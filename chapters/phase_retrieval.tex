

We enter now the core topic of the thesis. Most of the efforts during this PhD have been dedicated to the study of the Phase Problem 
for Bragg Coherent Diffraction Imaging using DL based approaches. Here I will discuss the main steps of this
journey, starting off from the analysis of the most relevant works in literature and concluding with some comments on the final version
of a DL model for highly strained particles. The latter has become the subject of an article, currently submitted, 
entitled ``\textit{Phase Retrieval of Highly Strained Bragg Coherent Diffraction Patterns with Supervised Convolutional 
Neural Network}'' \cite{Masto2025}. The process that led to the final version of the model will be unraveled, and particular attention
 will be given to elucidating the key steps and the critical issues encountered along the way. 
The structure of the chapter can be summarized as follows: 

\begin{itemize}
    \item \textbf{State of the art} and \textbf{Reciprocal space phase phasing}: introduces the works on DL for phase 
    retrieval in BCDI and discusses the novel concept of the prediction, using DL, of the reciprocal space phase (RSP) 
    lost during the measurement. 

    \item \textbf{2D case} and \textbf{Weighted Coherent Average loss function}: presents the preliminary studies on 
    the 2D case, for low-strain and high-strain simulated particles. Introduces the novel ``Weighted Coherent Average'' loss function, 
    designed specifically for the prediction of complex phases.

    \item \textbf{3D case - Patching approach}: discusses a patching-method for the low-strain and high-strain cases in 
    3D. 
    
    \item \textbf{Final 3D model}: presents the final DL model for the prediction of the RSP of highly strained 3D BCDI 
    patterns. Results on simulated and experimental data are shown as well as the combination of DL prediction and 
    iterative algorithms for refinement. 

    \item \textbf{Performance assessment}: the final DL model and standard algorithms are tested on simulated data with 
    different strain distributions to assess the benefits of the DL approach. 

\end{itemize}

\newpage

\section{State of the art}\label{chp:phasing_stateart}
In this paragraph I will focus on the state of the art for what concerns the Phase Retrieval of BCDI diffraction patterns with
deep-learning, tensor-computation and automatic differentiation methods. Conventional phase retrieval iterative algorithms 
are discussed in the introduction chapter as well as other approaches. \\
Given the relatively new development of neural networks and more specifically even more recent for BCDI phase retrieval, I will try
to give a chronological overview of the main works in the literature pointing out strengths and weaknesses.
The use of deep neural networks for inverse problems in imaging was proven to be successful in lensless computational 
imaging by Sinha in 2017 \cite{Sinha:17} and holography by Rivenson \cite{Rivenson2018}.
However, the first work pioneering the field for BCDI is ``Real-time coherent diffraction inversion using deep generative networks'' published
by Cherukara \textit{et. al} in 2018 \cite{cherukara_real-time_2018}. The paper presents two CNNs for the phase retrieval of small ($32\times32$ pixels) 2D 
simulated BCDI patterns, one predicting the support and the other the phase. A U-Net like architecture with 
encoder-decoder was implemented, and the model was trained for 10 epochs in a supervised fashion with a cross-entropy loss function (see Appendix).
The results showed an excellent agreement between prediction and ground truth also in presence of relatively strong phases. 
The potential of this new approach for phase retrieval becomes immediately clear when considering the drastic reduction of
computational time and resources needed for the model inference. Once the model is trained, the reconstruction can be obtained
within few milliseconds on a desktop machine. 

In 2020 Scheinker and Pokharel proposed another approach \cite{scheinker_adaptive_2020}
that employs a CNN model for 3D diffraction patterns. The fundamental difference is that the object's support was defined 
by its surface only, as it is assumed to be \textit{compact} and \textit{homogeneous} inside. Moreover, the surface was
parametrized by spherical harmonics and the DL model was trained to predict 28 of the first even coefficients of the spherical
harmonics. The model architecture was therefore essentially different since, while the encoder is just transposed to a 3D 
one, the decoder is replaced by a flattening and dense layer with 28 different classes as output. The model showed good performance
on both simulated and experimental data, marking the first DL-based approach capable of real 3D BCDI phase retrieval.
In the same year, Wu and coauthors, \cite{Wu2021}, opted for an architecture made of a single encoder and two identical decoders for the prediction of 
amplitude and phase of single crystals from the central slice of the BCDI pattern. They conducted the study on simulated 
data and tested it on one experimental case as well. What is evident from their work is the winning combination of DL prediction
and iterative refinement. The speed and generalization capabilities of the CNN allows for fast and good estimations of the 
object's support and phase. In addition, the precise and well established iterative methods can bring this initial guess to a 
more polished and accurate solution in fewer cycles than without DL prediction. This successful combined approach has been 
later adopted in other works, ours included. 

In 2021 two important works were published. First, Chan \textit{et al.} in 
\cite{chan_rapid_2021} extended the encoder/2-decoders architecture to the 3D case. In their work they first created a 
``physics-informed'' training set obtained building particles by clipping planes from a cubic FCC structure of atomic 
positions, relaxing them with LAMMPS software for molecular dynamics and computing the BCDI pattern around the (111) Bragg 
peak. The procedure is very similar to the one adopted by Lim \textit{et al.} in \cite{lim_convolutional_2021} and described
above in Section \ref{sec:dataset_creation3D}. Training the CNN on a restricted set of such created BCDI patterns biases 
the predictions towards physically meaningful particles. Moreover, it is interesting to notice that the training of the model 
was conducted in a sort of unsupervised fashion as the loss function calculates the differences between the target diffracted
intensity and the intensity obtained by the squared modulus of the discrete Fourier transform of the predicted complex object.
Although the authors managed to successfully test their model on
an experimental BCDI pattern, the small size ($32\times32\times32$ pixels) of the images accepted by the CNN was not yet 
enough for proper experimental use. The work of Wu \textit{et al.} \cite{wu_three-dimensional_2021} published 
in the same year, lifted the size to 64 pixel-sided cubes, enabling their model to be tested on several experimental cases. 
Their CNN model maintained the encoder/2-decoders architecture for a simultaneous prediction of the object's amplitude and phase 
and explores for the first time the unsupervised training for refinement as well. The authors claimed that this approach is 
able to achieve better reconstruction quality with respect to current state-of-the-art iterative algorithms in use. 
The year after, Yao and coauthors published AutoPhaseNN \cite{yao_autophasenn_2022}, again an encoder/2-decoders architecture
that completely trained in an unsupervised manner. This approach is beneficial as it doesn't require datasets labeled with 
a ground truth, which means that experimental data can be directly used in the training set. Another advantage is that it 
relaxes the limitation of simulating a sufficiently diverse population of samples, capable of constituting a comprehensive 
distribution of real cases. AutoPhaseNN was trained to predict an object the diffracted intensity of which matches the observed
one according to a normalized Mean Absolute Error metric. The model showed to work on simulated data as well as on experimental 
data and once more the winning method lies in the combination of DL prediction and iterative refinement. 
AutoPhaseNN has marked a milestone in the BCDI data analysis, attaining 10X to 100X phase retrieval speed up with reduced efforts 
for the model training. 

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{figures/Phasing/autophasenn.pdf}
    \caption{\textbf{Schematic of the AutoPhaseNN model for unsupervised BCDI Phase Retrieval.} \textbf{a} The convolutional 
    neural network is divided into an encoder and two parallel decoders for object's modulus and phase respectively. \textbf{b} 
    The loss function is calculated in reciprocal space, between the observed and calculated intensities, therefore enabling 
    unsupervised training. Adapted from \cite{yao_autophasenn_2022} }
    \label{fig:autophasenn}
\end{figure}

Although of different nature, it is worth mentioning the work of Zhuang and coauthors \cite{Zhuang2022PracticalPR} in which 
two CNNs are used in the ``deep image prior'' (DIP) framework. DIP \cite{Ulyanov_2020} typically implies the use of a CNN for 
an enhanced representation of an image, often to solve inverse problems like super-resolution, de-noising and inpainting. 
However, it differs from classical deep learning as there is no training dataset but a fit of the target problem exploiting
the parameters of the convolutional layers and the efficient gradient descent provided by the automatic differentiation. 
In their work, Zhuang \textit{et al.} formulated the more general far-field phase retrieval problem as an optimization problem 
and considered the phase symmetries that affect this class of solutions \cite{tayal2020} (see Introduction chapter). Their work employs two 
DIPs, one for the modulus and one for the phase, and successfully manages to reconstruct simulated objects even in presence 
of strong phases. 
The last interesting contribution is the work of Yu and \textit{et al.} \cite{yu_ultrafast_2024}. In this paper the authors
proposed a DL model that computes complex convolutions, handling real and imaginary parts of the complex tensor in a single
passage through the convolutional block. Complex convolutional layers are claimed to be better at preserving the physical connection between real and imaginary parts  
inside the complex object. Moreover, the authors made use of \textit{skip connections} between encoder and decoder to 
enhance the training. This is a rather peculiar as this kind of residual links are typically used, in convolutional 
encoder-decoder networks, for tasks in which the input and output images are visually similar (i.e. segmentation, denoising, inpainting), 
thus, where it is more evident the information flow from the two blocks of the network. 
The model was used for the phase retrieval of experimental 2D diffraction patterns, for which an 
unsupervised refinement was used as well. \\
Before proceeding with our study, Table \ref{table:models} summarizes the key features of the works from the two 
leading BCDI research groups at Brookhaven and Argonne National Laboratories, highlighting similarities and 
differences to guide the development of our model.

\begin{table}[ht]
    \centering
    \scriptsize
    \renewcommand{\arraystretch}{1.5} % increase vertical spacing
    \begin{tabular}{l|P{3.2cm}|P{2.2cm}|P{4.3cm}|P{3cm}}
    \textbf{} & \textbf{Architecture} & \textbf{Last Activation Layer} & \textbf{Loss Function} & \textbf{Refinement} \\
    \hline
    Cherukara - 2018 \cite{cherukara_real-time_2018} & Two different UNets & Sigmoids & Cross Entropy & - \\
    Wu - 2020 \cite{Wu2021} & Encoder / 2 Decoders & ReLU & MSE on mod and phase + PCC on magnitudes & Iterative \\
    Chan - 2021 \cite{chan_rapid_2021} & Encoder / 2 Decoders & ReLU & MAE on normalized magnitudes & Automatic Differentiation \\
    Wu - 2021 \cite{wu_three-dimensional_2021} & Encoder / 2 Decoders & LeakyReLU & MSE on mod and phase + PCC on magnitudes & Transfer learning + unsupervised training \\
    Yao - 2022 \cite{yao_autophasenn_2022} & Encoder / 2 Decoders & Sigmoid and Tanh & MAE on normalized magnitudes & Iterative (50 ER) \\
    Yu - 2024 \cite{yu_ultrafast_2024} & Complex encoder-decoder + skip connections & ReLU & MAE on real + MAE on imaginary & Transfer learning + unsupervised training
    \end{tabular}
    \caption{Comparison of deep learning-based phase retrieval approaches.}
    \label{table:models}
\end{table}

First, it is interesting to notice that the architecture's choice, from treating the object's modulus and phase separately 
with two different detached networks, moved over the years to a single ``standard'' U-Net that accounts for the complex 
nature of the data. Second, I noticed that the choice of the last activation layers, which are the ones producing the 
modulus and phase outputs, in their final value range, is not uniform throughout the articles. ReLU and sigmoid activations 
produce strictly non-negative outputs, making them naturally suited for quantities that are inherently positive, such 
as the modulus. In contrast, LeakyReLU and Tanh can generate negative values, which makes them valid choices for 
representing complex phases. However, the practical impact of this choice appears limited: in some cases, models can 
predict correct moduli using LeakyReLU and correct phases using ReLU or sigmoid. This can be explained by the fact 
that a global offset in the phase—shifting the entire range to positive values—does not change the physical solution. 
Consequently, a ReLU can produce a valid phase array, simply offset by a constant, and similarly for the sigmoid, 
provided the phase range fits within the activation's output limits. 
\\
One of the most important components of the model is the loss function, as its gradients regulate the model's 
parameters, hence the prediction. Except the first work that employs a cross entropy loss, normally 
used for classification tasks, other works opt for Mean Absolute Error (MAE) and Mean Squared Error (MSE), of standard 
use for regression and Pearson Correlation Coefficient (PCC) as well. Typically, 
when the loss is calculated between intensities the MAE and the PCC are used as they are more suitable for the high dynamic 
range of the diffraction patterns. MSE in fact, ``would overly de-emphasize errors in mid-intensity regions of the images''
\cite{chan_rapid_2021}.
Lastly, I have listed the different ways used to refine the DL predictions. Here we can notice that very soon GPU accelerated
gradient descent methods have been used in replacement of conventional iterative algorithms. The unsupervised training
allows to easily switch from inference to refinement using the same model in the same GPU optimized 
computing environment guaranteed by machine learning libraries like PyTorch and Tensorflow. 

\section{Reciprocal space phasing}\label{chp:phasing}

Taking inspiration from these works, but 
significantly changing the perspective, it was decided to predict the ``reciprocal space'' phase (RSP) that is 
lost during the measurement of the BCDI pattern rather than the complex object in real space.
The main, intuitive, reason behind this choice is the visual 
similarity between the morphology of the diffraction pattern and its corresponding RSP. 
Furthermore, it is common that many samples studied with BCDI have facets that happen to be, to some degree, parallel with each other, 
thus interfering like a single slit, with the typical fringes of intensity that correspond to constructive interferences, 
interspersed with dark regions arising from destructive interferences. In these specific cases, the RSP shows a regular 
pattern in which there is always a $\pi$ shift between two crests of the fringes as shown in Fig.\ref{fig:rec_space_phase}.
Once the RSP is retrieved, one can then recompose the full complex diffracted wave-function and obtain the complex object via 
inverse Fourier transform.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{figures/Phasing/rec_space_phase.pdf}
    \caption{Central slice of a typical BCDI pattern (\textbf{a}) with the corresponding RSP (\textbf{b}) obtained after a 
    successful reconstruction of the object (modulus and phase in \textbf{c - d} respectively). It highlights the structural 
    similarity between the diffracted intensity 
    in logarithmic scale and the RSP. Moreover, one can notice that in this case of low strain faceted particle, 
     the RSP varies regularly between 0 and $\pi$ (or $-\pi$) in correspondence of the intensity fringes. }
    \label{fig:rec_space_phase}
\end{figure}

Besides, given this ``simple'' law of constructive-destructive interferences, we hypothesized the possibility to predict patches 
of this RSP given a portion of diffraction pattern and then, similarly to the inpainting case, stitch them together
and obtain the full RSP. This entails a number of complications related to the so-called phase symmetries that I have encountered
during the development of the algorithms and that will be discussed in the next sections. \\
Ultimately, the goal of this DL model for phasing is to facilitate the reconstruction of highly strained particles. While 
other works in literature have mostly leveraged the gain in computing time, here the model aims at tackling those reconstructions for which 
conventional algorithms struggle to find convergence because of the high strain in the particle.   
However, in this case, the aforementioned RSP $\pi$-shifts in between two fringes is much more complicated since the 
strong and extended displacement fields inside the crystal alter the Bragg peak, merging and spreading the fringes 
into an irregularly distributed intensity pattern (see Fig \ref{fig:cube_strain}).

% al punto che non si e' in grado di stabilire, a occhio, se l'informazione necessaria per costruire la mappa I-phi e' racchiusa in una porzione 
\section{Dataset creation} 

I have trained our model in a supervised manner, meaning, in this case, that the training was always conducted on simulated data 
only, as the RSP is never experimentally detectable. 
For this reason, I have simulated the training dataset following the same procedure described in Sections 
\ref{sec:dataset_creation2D} and \ref{sec:dataset_creation3D} for the 2D and 3D cases, respectively. However, in this
case, the dataset size was $64\times64\times64$ pixels and the simulated RSP was used as the ground truth label for training.\\

I will anticipate here that for the high strain case I created a dedicated training set simulating the strain by applying 
an artificial ``strong'' phase to the particles. In order to have a diverse population of strain distributions I have 
simulated each object's phase using different functions and parameters, namely: with the sum of two Gaussian functions,
with the sum of two cosine functions and using a correlated Gaussian random field \cite{Gaussian_noise1984}. In each 
case, centers, amplitudes, variances, frequencies, and correlation lengths were randomly chosen to ensure a phase 
variation within the particle ranging between 
$2\pi$ and $5\pi$. By doing this, strongly distorted BCDI patterns, similar to experimental high-strain ones, were obtained. 
In particular, the two Gaussian functions phase can closely emulate the effect of the substrate induced strain inside 
Winterbottom particles. 


Similarly to the inpainting case, the BCDI patterns have been transformed into logarithmic scale and normalized between 
0 and 1. Batches of 32 images at the time were used. 

\section{2D case low strain}\label{chp:2d_nostrain}
% MODEL 2D CASE NO STRAIN: SHOW THE MODEL PREDICTING THE PHASE AND USE A LOSS ON THE FOURIER TRANSFORM 
Alike the inpainting case, I have first conducted some preliminary studies in 2D, on noise-less low strain data. Here I will 
briefly show the model's architecture, the loss function and the results. 
\subsection{Model structure}
The architecture that I used has a U-Net like structure with an encoder and a decoder. 
The encoder is composed of six convolutional blocks through which the input diffracted intensity is progressively 
reduced from the 64 pixel-side squares to a 1D flattened vector. Each convolutional block is composed of a convolutional 
layer, a LeakyReLU activation function and a MaxPooling layer that halves the feature's map dimensions. (illustrate the 
parameters later). \\
At the end of the encoder the so-called bottleneck, composed of a convolutional layer followed by a LeakyReLU activation, 
processes the feature map before passing it to the decoder which by means of transposed convolutions, LeakyReLU activations 
and UpSampling layers, brings back the feature map to the input's size. Skip connections between encoder and decoder blocks 
are employed as well. The output tensor is the result of a last single-channeled convolutional layer with no activation function. 
In this way we let the model predict unbounded tensors to account for the phase symmetries. 

\subsection{Loss function}
The choice of the loss function was first based on what was used in literature. 
A sum of the MSE computed on the objects' amplitudes and one on the phases has thus been used (Eq. \ref{eq:loss}). The ground truth 
objects were indeed available from the simulated data while the predicted objects have been first calculated with a 2D  
inverse Fourier transform from the diffracted amplitude and the predicted RSP (Eq. \ref{eq:ft_2D}). 
\begin{equation}
    \hat{O}(\mathbf{r})
    \;=\;
    \mathcal{F}^{-1}\!\bigl\{\sqrt{I(\mathbf{q})}\,e^{\,i\,\varphi_{\mathrm{pred}}(\mathbf{q})}\bigr\}(\mathbf{r})
    \quad,
    \label{eq:ft_2D}
\end{equation}

\begin{equation}
    \mathcal{L}
    \;=\;
    \frac{1}{N}\sum_{\mathbf{r}}
    \Bigl(\bigl|\hat{O}(\mathbf{r})\bigr|
        \;-\;\bigl|O(\mathbf{r})\bigr|\Bigr)^{2}
    \;+\;
    \frac{1}{N}\sum_{\mathbf{r}}
    \Bigl(\phi(\mathbf{r})
        \;-\;\phi_{\mathrm{gt}}(\mathbf{r})\Bigr)^{2}
    \quad,
    \label{eq:loss}
\end{equation}

\subsection{Results}
The training of the model was conducted on 8500 simulated BCDI patterns over 30 epochs with a learning rate of 0.0003
and monitored both training and validation loss. Here, Fig.\ref{fig:loss_2mse_nosymm} shows the model's loss during the 
30 epoch long training. 

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{figures/Phasing/loss_low_strain_noiseless_doubleMSE_nosymm.pdf}
    \caption{Training and validation loss over 30 epochs. The curve suggests a proper learning with no overfitting as 
    both losses are decreasing reaching a plateau and the validation loss follows the same trend of the training loss.}
    \label{fig:loss_2mse_nosymm}
\end{figure}

Fig.\ref{fig:RSP_lowStrain_doubleMSE} illustrates the results of the predicted RSP of some test simulated BCDI patterns. 
Note that the displayed predicted RSP has been wrapped between 0 and 2$\pi$ for better comparison with the ground truth 
but the raw output of the model is in fact an ``unwrapped'' array. This is expected since no activation layer was applied 
to the last convolutional layer, meaning that the last operation is the multiplication of the last feature map with the 
real values inside the convolutional kernel, hence linear. \\
When comparing the reconstructed objects obtained from the predicted RSP with the ground truth ones (Fig. \ref{fig:obj_lowStrain_doubleMSE})
one can draw some interesting conclusions about the model's learning performance. 
First it can be observed that the model learns the approximate shape and size of the particle, it produces images 
that resemble reasonable particles, sometimes similar to the ground truth. The amplitude is concentrated inside 
the support with little noise outside and the phase is overall correct around zero. \\

However, when looking more carefully, 
it is clear that the shape is not quite correct, especially for highly non-centrosymmetric objects. For instance, if we 
consider the object in Fig. \ref{fig:obj_lowStrain_doubleMSE} \textbf{c}, we see that the predicted shape seems to be 
deriving from the incorrect superposition of the correct shape and its twin, as well correct. More in general it seems 
that the model tends to predict centrosymmetric objects. According to Sicairos \textit{et al.} \cite{guizar-sicairos_understanding_2012}, 
if we name $\varphi(\vec{q})$ the correct RSP, this phenomenon arises from a predicted RSP phase $\phi$ composed 
of $\varphi(\vec{q})$ in some regions of the $q$-space and $-\varphi(\vec{q})$ elsewhere. In other words, the model is not 
fully able to break the sign symmetry. This subject was recently studied by  Tayal \textit{et al.} \cite{tayal2020} and Zhang 
and coauthors in \cite{zhang_what_2024}. 
In this last study, the authors show that if not broken in the dataset, meaning that during the 
training the model is exposed to both cases ($\varphi(\vec{q})$ and $-\varphi(\vec{q})$) indistinctly, the model is deceived 
to produce a mix of the two, since the sign information cannot be recovered from the input intensity. The authors conclude that 
in order to prevent this detrimental effect, one should break the symmetry in the dataset to bias the model towards one 
preferred sign. 

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{figures/Phasing/RSP_low_strain_doubleMSE.pdf}
    \caption{\textbf{Model testing on new 2D data using MSE loss function}. First row shows four simulated BCDI patterns, second row the ground truth RSP 
    corresponding to the pattern and last row the DL prediction. The superposition of the two twin solutions resulting 
    in the object space can be inferred here by observing how iso-phase lines in the predicted RSP tend to be more 
    ``circular'', as it would be for round direct-space objects. Moreover, some squared artifacts are visible in the predicted
    RSP. Being the side of the square half the size of the image, one can hypothesize that is originated by an incorrect 
    up-sampling in the last convolutional layer.}
    \label{fig:RSP_lowStrain_doubleMSE}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{figures/Phasing/obj_low_strain_doubleMSE.pdf}
    \caption{\textbf{Corresponding reconstructed objects}. Ground truth and predicted objects' amplitudes (first two rows 
    respectively) and ground truth and predicted objects' phases (first two rows respectively). Note that the predicted 
    objects resemble the superposition of the two twin true solutions.}
    \label{fig:obj_lowStrain_doubleMSE}
\end{figure}


The procedure presented in the article for the removal of the phase symmetries consists in: (i) the centering of all the 
objects in real space (phase ramp removal), (ii) the shift of the RSP such that the $k$-th pixel for which $\varphi_k = 0$
is the same across the dataset (phase offset removal), and lastly, (iii) the flip of the sign of the whole RSP when its 
value corresponding to a fixed position across the dataset is negative ($ \text{if  } \varphi_{k+1} <0 \text{  then  } 
\varphi = -\varphi $). In our case the phase ramp symmetry was already broken by simulating 
particles with the center of mass in the center of the array. In this way the model is already biased towards the prediction 
of RSPs that yield centered objects. For the offset and the sign, the method proposed by Zhang \textit{et al.} has been 
implemented in the model and the results are shown in Fig. \ref{fig:RSP_lowStrain_doubleMSE_JuSun} for the RSP and 
Fig. \ref{fig:obj_lowStrain_doubleMSE_JuSun} for the reconstructed objects. 

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{figures/Phasing/RSP_low_strain_doubleMSE_symmJuSun.pdf}
    \caption{\textbf{Model testing using MSE loss function and biased dataset}. First row shows four simulated BCDI patterns, second row the ground truth RSP 
    corresponding to the pattern and last row the DL prediction . The results do not seem to improve since the predictions 
    look identical to the ones in Fig. \ref{fig:RSP_lowStrain_doubleMSE}.}
    \label{fig:RSP_lowStrain_doubleMSE_JuSun}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{figures/Phasing/obj_low_strain_doubleMSE_symmJuSun.pdf}
    \caption{\textbf{Corresponding reconstructed objects}. Ground truth and predicted objects' amplitudes (first two rows 
    respectively) and ground truth and predicted objects' phases (first two rows respectively). No significant improvement 
    can be observed after the adopted sign-symmetry breaking procedure.}
    \label{fig:obj_lowStrain_doubleMSE_JuSun}
\end{figure}

Unfortunately, the proposed method did not seem to solve the sign ambiguity of the RSP. The model is still unable to 
discriminate between the plus/minus sign of the RSP and the result is the incorrect overlap of the object with its twin
obtained by the inversion symmetry. The phase, though small for this case, is also showing a kind of centro-symmetry 
as its variations tend to spread radially from the center of the array. 

\subsection{The Weighted Coherent Average loss function}

At this point in the study, and in anticipation of applying the model to portions of the RSP, it became necessary to 
consider a loss function that would operate directly on the phase, without requiring transformations into direct-space. 
However, the main challenges were posed by the symmetries inherent to the phase. Upon further reflection, it was concluded
that a Mean Squared Error (MSE) is not an appropriate metric for comparing the phases of complex functions. Indeed, MSE 
fails to account for the 2$\pi$ periodicity and the possibility of a global phase offset. One could argue that 2$\pi$ wraps 
can be fixed with a modulo 2$\pi$ operation and the offset can be removed by shifting the tensor by a constant. However, 
the modulo wrapping function jumps abruptly by 2$\pi$ every time phase crosses an integer multiple of 2$\pi$, meaning that 
the gradients are infinite thus not advised for gradient-based optimizations. Moreover, the MSE (or MAE and other 
\textit{divergent} metrics) will have problems at the 0-2$\pi$ boundary. In fact, when considering the phase mapped in the
0-2$\pi$ range, if we suppose a $\varphi_{pred}^0 = -0.1$ where $\varphi_{G.T.}^0 = 0$, the wrap will move the $\varphi_{pred}^0$ to the value 
$2\pi - 0.1 = 6.183$ amplifying the error ( $\Delta\varphi)$ from $0.1^2$ to $6.183^2$ improperly.\\

In order to bypass these shortcomings a new loss function was designed. Here it follows the reasoning process that leads
to the mathematical expression of the loss.  \\
The best way to account for the periodicity and the wrap, without discontinuities 
and error unbalances, is to evaluate the ground truth - predicted phase differences ($\Delta\varphi_k$) on the unit circle. 
To do such, it's necessary to express $\Delta\varphi_k$ as angles of a complex exponential. This means that if $\varphi_{pred}$ is an array of 
random values, each complex number $ z_k = e^{i\Delta\varphi_k}$, when represented on the Argand plane, can be seen as a 
vector pointing at a random coordinate on the unit circle (Fig. \ref{fig:WCA}\textbf{c}). Now, the goal of the optimization is not to minimize $\Delta\varphi_k$ 
for all $k$ but to have the same $\Delta\varphi_k$ throughout $k$. In fact, for $\varphi_{pred} \Leftrightarrow \varphi_{G.T.}$ each 
vector $z_k$ points in the same direction, but it does not necessarily lie on the x-axis ($\Delta\varphi_k = 0 $ condition). 
Therefore, the loss function should ultimately drive all the $z_k$ from randomly distributed to coherently aligned along a common 
direction. 

A helpful quantity in this case can be the complex average vector $\langle z \rangle = \frac{1}{N}\sum_{k=1}^{N}z_k = \frac{1}{N}\sum_{k=1}^{N}e^{i\Delta\varphi_k}$
where $k$ runs over all the $N$ pixels. In particular the length of $\langle z \rangle$ , represented by the modulus $|\langle z \rangle|$,
is an efficient metric for the measurement of the \textit{degree of alignement} of all the $z_k$ between each other. 
In other words it measures the ``coherence'' between $\varphi_{GT}$ and $\varphi_{pred}$.

In fact, $|\langle z \rangle|$ scores 0 for randomly oriented $z_k$, as opposite contributions cancel out each other because 
incoherent, while it scores 1 for perfectly aligned ones. It follows that one wants to maximize $|\langle z \rangle|$ during the 
optimization. Moreover, given the natural normalization between 0 and 1 of this metric, it follows naturally that the loss 
function can be expressed as $ L =  1 - |\langle z \rangle| $. \\

Additionally, an importance mask can be applied during the averaging process. In particular, we know that the brightest 
pixels of the BCDI pattern are the ones contributing the most to the object's reconstruction. For this reason one could 
weight the complex average by multiplying by the input magnitudes. The effect of this operation is to ``give a direction'' to the 
optimization, meaning that the $\langle \Delta\varphi \rangle $ the model will tend to converge to, will be mostly steered close to 
the $\Delta\varphi_k$ of the brightest $k$ pixels. 
The loss can now be expressed as: 

\begin{equation}
    L = 1 - \left|\frac{1}{N}\sum_{k=1}^{N} \sqrt{I_{k}}\exp\left(i(\varphi_{\text{GT},k} - \varphi_{\text{pred},k})\right)\right|
\label{eq:WCA_1}
\end{equation}

Where $N$ is the total number of pixels in each RSP array and $k$ is the pixel index. $\sqrt{I}$ is the magnitude of the BCDI pattern 
normalized between 0 and 1 with respect to the sum, and $\varphi_{\text{GT}}$ and $ \varphi_{\text{pred}}$ the ground truth and 
predicted RSP. \\
The last missing piece is the removal of sign symmetry. Rather than biasing the dataset preferring one sign over the opposite, 
the function $L$ is computed for both $\varphi_{\text{GT}}$ and $-\varphi_{\text{GT}}$ and in a second passage, the minimum of the two 
along the batch dimension is kept for backpropagation. The final form of the Weighted Coherent Average (WCA) loss is then given 
by: 

\begin{equation}
    L_{\text{WCA}} = \min\left(L_+, L_-\right)
\label{eq:WCA_2}
\end{equation}

To better visualize the functioning of the WCA loss function, a simple model has been trained to fit the ground truth phase 
of a single 2D BCDI pattern using the WCA. The complex phase differences vectors were extracted at each step of the optimization 
together with the updates obtained from the gradients of the WCA with respect to the trainable parameters. Fig. \ref{fig:WCA} 
shows the evolution of the predicted RSP as well as the progressive alignment of the  $z_k$.
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{figures/Phasing/WCA.pdf}
    \caption{\textbf{Illustration of the WCA loss function}. \textbf{a} Input intensity (log-scale normalized) and
    ground truth RSP. \textbf{b} Predicted RSP in steps 0 - 150 - 400 of the optimization. \textbf{c} Corresponding 
    complex phase-differences vectors $z_k$ on the Argand plane (blue arrows), together with the updates (green arrows) obtained 
    from the gradients of the WCA, and the resultant complex average $\langle z \rangle$ (red arrow). It is visible that 
    during the fit, as the $z_k$ align around a common one, the amplitude of $\langle z \rangle$ grows bigger and the predicted 
    RSP converges to the ground truth one. Notice in \textbf{c} the vectors align around a $\Delta\varphi \neq 0$ resulting 
    in a $\varphi_{pred}$ shifted by a constant offest. The offset has been removed before plotting in \textbf{b} for 
    better comparison with the ground truth.}
    \label{fig:WCA}
\end{figure}

The same model has been trained using the WCA for the same number of epochs on the same dataset and here the results are 
shown. First, it can be noted in Fig.\ref{fig:loss_vfn} that the training and validation loss values throughout the 
training are following different trends with respect to the model trained with the MSE loss (Fig. \ref{fig:loss_2mse_nosymm})

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{figures/Phasing/loss_low_strain_noiseless_doubleVFN.pdf}
    \caption{Training and validation loss curves over 60 epochs using the WCA loss function.}
    \label{fig:loss_vfn}
\end{figure}

In this case the correct learning curve does not reach a plateau within the first 25 epochs but maintains a negative slope 
for longer, indicating a better learning. This suggests indeed better results when used on test data. 
In particular, for the same input diffraction patterns tested above in Figs.\ref{fig:RSP_lowStrain_doubleMSE_JuSun} - \ref{fig:obj_lowStrain_doubleMSE_JuSun}
the model trained with the WCA yields the prediction shown in Fig.\ref{fig:RSP_vfn} for the RSP and Fig.\ref{fig:obj_vfn} 
for the corresponding reconstructed objects. 

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{figures/Phasing/RSP_low_strain_VFN.pdf}
    \caption{\textbf{Model testing using WCA loss function}. First row shows four simulated BCDI patterns, second row the ground truth RSP 
    corresponding to the pattern and last row the DL prediction. Here the squared artifacts disappeared, suggesting 
    a better learning. }
    \label{fig:RSP_vfn}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{figures/Phasing/obj_low_strain_VFN.pdf}
    \caption{\textbf{Corresponding reconstructed objects}. Ground truth and predicted objects' amplitudes (first two rows 
    respectively) and ground truth and predicted objects' phases (first two rows respectively). Here the twin problem 
    mentioned above and visible in Figs.\ref{fig:obj_lowStrain_doubleMSE} - \ref{fig:obj_lowStrain_doubleMSE_JuSun} 
    is less pronounced, the model seems to manage the two solutions, often preferring the twin image to the ground truth. }
    \label{fig:obj_vfn}
\end{figure}

The results obtained from the model trained with the WCA loss function are visually better than the MSE ones. Although not 
completely removed, the sign symmetry that gives rise to the superposition of the object with its twin, is less pronounced. 
For example, particles in Fig. \ref{fig:obj_vfn}(a-b-d) have a clear orientation and a shape that matches the ground truth. 
In all those cases though, the model has opted for the conjugate solution as the predicted object are flipped with respect to 
the ground truth ones. In Fig. \ref{fig:obj_vfn}(c) instead the symmetry is not broken and the result is still a superposition 
of the particle with its twin. This suggests that the symmetry breaking method implemented in the WCA, and the one proposed by 
Zhang and coauthors, is only partially playing a role in the actual model learning. It is interesting to notice indeed that 
when the training dataset or the model trainable parameters are increased, the sign symmetry is completely removed in the most 
difficult cases as well. Fig. \ref{fig:loss_comparison} shows the effect of the dataset and models sizes for both MSE and WCA 
loss functions on the same simulated test data. The first important piece of information this figure shows is that the model trained 
with the WCA reaches higher accuracy. Moreover, it is much faster to compute since no FFT or IFFT is involved (see Fig. \ref{fig:loss_comparison}), 
thus the training time is drastically reduced. For what concerns the accuracy metric, in order to properly account for 
both modulus and phase, it has been calculated using 
\begin{equation}
 \left(\frac{PCC(\rho_{pred}, \rho_{GT}) + WCA(\rho_{pred},\phi_{pred},\phi_{GT} )}{2}\right)\times 100
 \label{eq:accuracy_phasing}
\end{equation}
where $PCC(\rho_{pred}, \rho_{GT})$ is the Pearson Correlation Coefficient on the object's moduli and $WCA(\rho_{pred},\phi_{pred},\phi_{GT} )$ 
is the WCA function applied to the object's phase weighted by the normalized predicted modulus (Eq.\ref{eq:WCA_1}-\ref{eq:WCA_2}). 
For what concerns the sign symmetry problem it is evident that while for 
the MSE trained model it is resolved only for a larger number of trainable parameters, for the WCA trained one it is already 
sufficiently overcome. Finally, it is interesting to notice that when the model size is kept fixed and the training 
dataset augmented, the WCA improves the performances while for the MSE it is not the case. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/model_comparison.pdf}
    \caption{\textbf{Comparison of MSE and WCA loss function for different model and training dataset sizes} In the first 
    row from left to right the input intensity, the ground truth RSP and the corresponding object (modulus and phase) are 
    represented. \textbf{$a_0, b_0, c_0$}} are the results of the predicted RSP obtained from the model trained with the 
    MSE loss function with the initial number of parameters and training set (\textbf{a}), with the augmented dataset (\textbf{b})
    and with both model and dataset size increased (\textbf{c}) In third and fourth rows the corresponding reconstructed 
    objects are displayed. \textbf{$d, e, f$} columns symmetrically shows the results obtained with the model trained using the WCA loss. 
    The WCA yields better results and requires shorter trainings. 
    \label{fig:loss_comparison}
\end{figure}

\section{2D high strain case}

In this paragraph the model training was performed on a dataset of highly strained 2D BCDI pattern simulated as described above  
in section \ref{sec:dataset_creation2D}. In this case Poisson statistic was applied to each dataset to better simulate 
the experimental condition. A set containing 30'000 samples was created and the ``bigger'' model of 27.5M parameters mentioned in Fig 
\ref{fig:loss_comparison} was trained over 50 epochs with a learning rate of 0.001. Similarly to the low-strain case 
described above the same model has been trained with the MSE and WCA losses separately for comparison. The goal of this 
study is in fact to test the relevance of the loss function compared to the size of the model when the complexity of the 
task increases. The results on 4 test BCDI pattern are shown in Figs. \ref{fig:2D_hs_MSE_RSP}-\ref{fig:2D_hs_MSE_obj} for the MSE
one and \ref{fig:2D_hs_VFN_obj}-\ref{fig:2D_hs_VFN_obj} for the WCA.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/2D_hs_MSE_RSP.pdf}
    \caption{RSP predicted by the model trained with the MSE loss function calculated on both modulus and phase of the 
    reconstructed object. First row: simulated 2D strained BCDI patterns (test dataset). Second row: 
    corresponding ground truth RSP. Third row: predicted RSP. Once can notice that the model cannot predict correctly the 
    RSP where the ``iso-phases'' do not have a circular symmetry (see \textbf{b-d}).} 
    \label{fig:2D_hs_MSE_RSP}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/2D_hs_MSE_obj.pdf}
    \caption{Reconstructed objects corresponding to Fig \ref{fig:2D_hs_MSE_RSP}. First and third row: ground truth modulus and phase. Second and fourth row: 
    model's results of objects' modulus and phase. Although the shape might at first sight look like the ground truth one (or 
    the twin) the phase is often incorrect. It is curious to notice that better results are obtained when the object's 
    phase possesses a certain degree of symmetry with respect to the center, analogously to the corresponding RSP. } 
    \label{fig:2D_hs_MSE_obj}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/2D_hs_VFN_RSP.pdf}
    \caption{RSP predicted by the model trained with the WCA loss function. Here the model correctly retrieves the RSP for 
    non-circularly symmetric structures as well (\textbf{b-d})} 
    \label{fig:2D_hs_VFN_RSP}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/2D_hs_VFN_obj.pdf}
    \caption{Reconstructed objects corresponding to Fig \ref{fig:2D_hs_MSE_WCA}. Compared to the objects obtained 
    from the DL model trained with the MSE loss function (Fig.\ref{fig:2D_hs_MSE_obj}) here the objects' phases are 
    correctly recovered, also when the symmetry is not circular (\textbf{b}).} 

    \label{fig:2D_hs_VFN_obj}
\end{figure}


The preliminary studies on the 2D case for low-strain particles have demonstrated the possibility to recover the RSP from 
the diffracted intensity pattern with a U-Net like architecture without ever calculating the object in real space. Moreover, 
the studies on the high strain particles has shown that this model configuration is well suited for this case as well.
From these promising results, it was decided to investigate the mapping intensity-RSP for patches of the reciprocal space. 


\section{Phasing patches: 3D case low strain}\label{chp:patches_nostrain}
% MODEL 3D CASE NO STRAIN WITH PATCHES: SHOW THE MODEL WITH DIFFERENT WCA LOSS FUNCTION 
In this section of the manuscript the DL prediction of ``patches'' of the RSP will be explored and discussed. 
Three-dimensional BCDI pattern of low strained particles were used to conduct this study. 
Although this patching approach has not given satisfactory results for the PR, it is nevertheless reported in the manuscript as 
study on the \textit{local} rather than \textit{global} relationship between the diffracted intensity and the RSP. It is 
indeed known that a unique mapping exists, barring some trivial RSP symmetries, between the diffracted intensity 
and the RSP in 3D \cite{Miao:98}. What is interesting to investigate is whether this relationship exists also for subsets 
of the reciprocal space, and in particular if it can be retrieved by a DL model. (From now on the term ``patches'' 
will be used to refer to cubic subsets of the reciprocal space). \\

When deciding to work with patches, there is a number of questions that arise and the answer to which is not 
straightforward nor unique in many cases. Namely: 
\begin{itemize}
    \item What size is best? 
    \item Can the patches be extracted at random positions or should there be an order? 
    \item What about the normalization of the intensity range inside the patch? 
    \item How are the patches stitched together into the full RSP eventually? 
    \item How are the phase symmetries taken into account during the stitching? 
\end{itemize}
Here I will present the approach that allowed me to address these questions. 

\subsection{The choice of the size}

Similarly to the inpainting case, 32 pixel-side cubic patches, cropped out of 128 pixel-side simulated
BCDI patterns were considered. The choice was supported by the following reasons: 

\begin{itemize}
    \item The good results obtained for the inpainting case suggested that the amount of information contained inside a
    32 pixel-side patch of reciprocal space is enough for the model to grasp spatial correlations.  
    \item The average oversampling ratio of BCDI experimental data is such that in a 32 pixel-side volume a sufficient
    amount of fringes is contained, meaning intuitively that the model can predict the corresponding RSP. 
    \item An even number multiple of 2 is usually considered GPU-friendly since it facilitates the shared calculations across
    different threads. 
\end{itemize}

\subsection{Patches division and stitching}\label{sbsec:patches_creation}
At first, the patches were extracted randomly from the full BCDI pattern as for the inpainting case. 
However, by doing so the RSP of each patch would in principle have different offsets and different wraps than the neighbors 
and this would complicate the stitching of the patches back into the full RSP. 
For this reason, and considering the approximate spherical symmetry of the average BCDI pattern it was decided to 
crop patches radially, starting from the region around the center of the Bragg peak and the progressively moving 
outwards to higher q-values. 
In this configuration, an integer step (10 pixels in our case) was chosen beforehand and the first patch around the center 
of the Bragg peak was selected together with all the patches centered in distances of integer multiples of the chosen step. 
Fig. \ref{fig:patches_cropping} shows a simplified schematic of the patches' extraction. 
For the DL model training the patches of the intensity pattern need to be selected as well as the for the corresponding 
RSP for ground truth comparison. 


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/patching_cropping.pdf}
    \caption{\textbf{Schematic of the cropping of patches (not in scale)}. (\textbf{Left}) From the full BCDI pattern (white 128 px-side cube) the first patch is 
    cropped out in the center and (orange 32 px-side cube with blue outline). Other patches are extracted radially from 
    concentric shells separated by a 10 pixels distance. The gray shaded area highlights the overlapping volume between the 
    two patches. \textbf{Right} The projection view shows the full first shell and two patches from the second shell 
    for simplicity. 
     }
    
    \label{fig:patches_cropping}
\end{figure}

Being the step size smaller than the 
semi-diagonal of the 32 pixel-sided patches, it follows that the patches of adjacent cells have overlapping volumes. 
These common regions can have a twofold purpose. Firstly, they reduce the complexity of the stitching procedure since 
when this is executed progressively starting from the central patch, the sign and the offsets of the RSP are unambiguously 
fixed for all the following ones. Secondly, during the DL model training, for patches belonging to the outer shells, 
the overlapping volume of RSP belonging to the innermost adjacent shell can be provided as initial guess along with the input 
intensity patches. This of course cannot be exploited for the central patch that necessarily has to be predicted without 
initial RSP guesses. \\
The last question to be answered concerns the normalization. Since each patch is processed independently of the others 
by the DL model, it was decided to normalize each patch between 0 and 1 (always in log scale). 

To summarize, the final design implied the use of two distinct training datasets and two different CNNs. 
The first dataset was dedicated to the central portion, therefore the first CNN (\textit{Central-CNN}) was provided with 3D intensity patches in 
input (normalized log scale) and corresponding RSP patches as ground truth labels. A second training dataset containing 
patches from outer shells (5 concentric ones for a 128 pixel-sided full BCDI pattern) was created. Here each file was made 
of the pair intensity-RSP initial guess - from the closest neighbor patch belonging to the innermost shell -  as input,
and the full RSP ground truth patch corresponding to the input intensity. This second dataset was used to train a second 
CNN (\textit{Outer-CNN}) identical to the first one. One observation regarding the datasets is that there is an intrinsic imbalance between the 
number of central patches and the outer ones. In fact, for a single full BCDI pattern, the number of patches in the first shell is 1, 
while the number of outer portions can go up to several hundreds. Moreover, the central patch is the most important one as it 
contains a low resolution representation of the particle in real space. In order to balance the training, the first dataset was 
augmented with more simulated data. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/guess_RSP.pdf}
    \caption{Example of input-ground truth pairs for the outer patches model. \textbf{a} Central slice of the input intensity 
    for the patch cropped from the first shell at position [58,56,64] (the central patch is at [64,64,64]). \textbf{b} Initial 
    RSP guess deriving from the overlapping region of the intensity patch with the central one. The blank area represents 
    the part that needs to be predicted. \textbf{c} Ground truth RSP corresponding to the intensity patch in \textbf{a}.}
    
    \label{fig:patches_Xy}

\end{figure}

\subsection{Model architecture}\label{chp:3d_patch_model} 
The model architecture is similar to the one used for the inpainting case, with a U-Net like structure. 
5 encoder blocks reduce the feature map to 512 one-dimensional vectors in the bottleneck and 5 decoder blocks upsample 
the feature map back to the original size. Skip connections are used as well and similarly to the 2D case for Phase Retreival, 
the last layer has been left with no activation function. The total number of trainable parameters is of the order of 3.5 millions. \\
For the second CNN the layers and features are identical to the first one but three modifications were made. Namely:
\begin{itemize}
    \item The input tensor was composed of the intensity patch concatenated with the initial RSP guess and a binary 
    mask marking the RSP guess voxels from the ot hers.
    \item The mask was used at the exit of the decoder as well to 
    select the new predicted voxels only for the backpropagation. 
    \item The WCA loss function has been restricted only to the positive sign of the RSP since no sign ambiguity is left 
    when fixing an initial guess
\end{itemize}

\subsection{Results on central patches}

\textit{Central-CNN} has been trained over 50 epochs on 10000 samples of central $32\times32\times32$ pixel-size patches 
cropped out of $128\times128\times128$ pixel-size full BCDI simulated pattern using the WCA loss function. 
Fig.\ref{fig:loss_3D_lowstrain} shows a good learning curve which is supported by the results on test data, illustrated 
in Figs. \ref{fig:centralpatch_RSP_lowstrain} - \ref{fig:centralpatch_obj_lowstrain}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/loss_central_patch.pdf}
    \caption{Training and validation loss curves over the 50 epochs long training. The plots indicate a good learning 
    of the model because both training and validation losses are decreasing monotonically with the same pace.}
    
    \label{fig:loss_3D_lowstrain}

\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/central_patch_lowstrain_RSP.pdf}
    \caption{Slices of the central patches. First row shows four different examples of BCDI patterns cropped around the 
    center of the peak. The small oversampling ratio of $\mathbf{b}_0$ and $\mathbf{b}_1$ makes such that the central portion 
    already contains the full useful signal and shows the diversity of the dataset. Row from $\mathbf{a}_1$ to $\mathbf{d}_1$
    shows the corresponding ground truth RSP while the last row shows the RSP predicted by the DL model.  }
    
    \label{fig:centralpatch_RSP_lowstrain}

\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/central_patch_lowstrain_obj.pdf}
    \caption{Corresponding objects from the central RSP patches in Fig.\ref{fig:centralpatch_RSP_lowstrain}. First 
    and third rows show the modulus and the phase of the ground truth objects while
    second and fourth rows show the predicted ones. Although the low resolution due to the limited reciprocal space 
    window, the model proves to correctly find the shape and phase distribution of the particle. }

    \label{fig:centralpatch_obj_lowstrain}
\end{figure}

From the results obtained after the training of the model dedicated to the central portion of the simulated BCDI patterns, 
one can conclude that the model is capable of retrieving the correct RSP for the low strain case, meaning that the leap from 
the 2D case to the 3D case does not imply unforeseen complications. Moreover, given the diversity of the training dataset 
the model manages successfully for full peaks contained in a small patch. \\

\subsection{Results on outer patches}

\textit{Outer-CNN} was trained on patches extracted from outer shells over 50 epochs on a dataset containing 50000 samples. 
Fig. \ref{fig:outerpatch_obj_lowstrain} illustrates some relevant results. In particular one can observe that the model 
can predict the RSP in the missing regions providing a relatively smooth transition between the ``known'' and ``unknown''
parts. This result is particularly interesting as it proves that a CNN trained with the WCA loss function 
learns the map that links a portion of diffracted signal with the corresponding RSP with no information on the particle in
real space nor the position of this portion with respect to the center of the diffraction peak. It also shows that the 
size of the patches contains a sufficient amount of information for this map to be learned. 
However, one can also notice that some ``noise'' is present in the predicted regions. These discrepancies from the ground 
truth become detrimental during the stitching procedure. 

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\textwidth]{figures/Phasing/outer_patches_low_strain_RSP.pdf}
%     \caption{Examples of RSP prediction for outer patches. The first row shows the central slice of some patches extracted 
%     far from the central part of the BCDI peak. The second row shows the ground truth RSP, the third row the RSP initial guess 
%     obtained from the overlap with the nearest neighbor of the innermost shell, and the last row shows the 
%     DL output. The DL prediction is limited to the missing region. }

%     \label{fig:outerpatch_obj_lowstrain}
% \end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/outer_patches_3D_lowstrain.pdf}
    \caption{Examples of RSP prediction for outer patches. The first row shows the central slice of some patches extracted 
    far from the central part of the BCDI peak. The second row shows the ground truth RSP, the third row the RSP initial guess 
    obtained from the overlap with the nearest neighbor of the innermost shell, and the last row shows the 
    DL output. The DL prediction is limited to the missing region (in red). }

    \label{fig:outerpatch_obj_lowstrain}
\end{figure}

\subsection{Results on the full RSP - Stitching the patches}

Here the results of the combination of the two CNNs are presented for the low-strain case. Once completed the training 
the model was tested on full simulated and experimental BCDI pattern. In order to properly retrieve the full RSP corresponding 
to the diffracted intensity a stitching algorithm for the predicted patches was designed. The stitching takes place
progressively starting from the central patch and updating the full RSP array shell by shell. Once the prediction of the 
central patch RSP is obtained from \textit{Central-CNN}, the patches of intensity belonging to the first shell at distance 10 pixels 
are extracted and the overlapping regions between each patch with the central one are calculated. The predicted central patch RSP 
for each overlapping region is therefore located in the initial guess RSP array that is given to the model as input paired with 
the intensity patch. Subsequently, the full batch of pairs $(I,\varphi_{guess})_{shell = 1}$ is sent through 
\textit{Outer-CNN} and the corresponding RSP output is obtained. 
At this stage two main issues have to be considered, namely: (i) during the training of \textit{Outer-CNN}
the initial guess RSP was taken from simulated \textit{ground truth} RSPs while here it is taken from the model's 
previous prediction itself. It is for this reason that any small unavoidable discrepancies between the predicted and 
ground truth RSP can lead the model to further errors. (ii) When a round of RSP is predicted there are overlapping 
regions between patches of the same shell and patches of the previous batch. The most straightforward way to perform the 
stitching of the patches into the full RSP is to overwrite each time the results. However, although a better approach 
based on the average of the overlapping prediction was implemented, the issue did not seem to be fully resolved.
A first test was conducted with this simpler solutions for both issues, and a more robust approach is briefly discussed 
at the end of the section. \\

The crop-predict-stitch method is hence repeated until the last shell. The number of shells and predictions scales with 
the size of the BCDI data, and it is however always restricted to a spherical region around the Bragg peak. It is therefore 
less accurate for non-cubic data. 
Here, Fig.\ref{fig:stitching_sim_low} shows an example of full RSP stitching for a cubic 128 pixel-sided simulated BCDI 
pattern, while Fig.\ref{fig:stitching_exp_low} reports the analogous result for an experimental data phased with PyNX. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/stitching_low_strain_sim.pdf}
    \caption{\textbf{Example 1.} Results of the stitching of RSP predicted patches for 3D simulated data (central slice displayed). The 
    green square on the intensity figure represents the central patch.}
    \label{fig:stitching_sim_low}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/stitching_low_strain_exp.pdf}
    \caption{\textbf{Example 2.} Results of the stitching of RSP predicted patches for 3D experimental data (central 
    slice displayed). Here the ground truth quantities are obtained with PyNX phase retrieval of the diffraction 
    pattern. }
    \label{fig:stitching_exp_low}
\end{figure}

What emerges from Figs. \ref{fig:stitching_sim_low} - \ref{fig:stitching_exp_low} can be summarized as follows: \\
(i) the accurate prediction of the central RSP patch is fundamental to retrieve the low resolution estimate of the object \\
(ii) the stitching is problematic already from the first shell outside the central patch, most likely because of the 
two issues pointed out above (initial guess from model prediction and RSP averaging of overlapping regions)\\
(iii) the outer RSP patches correctly infer the oversampling ratio as the ``thickness'' of the RSP oscillations matches 
the one of the diffracted intensity, nonetheless, the preferred overall orientation of the fringes seems to be circular.  \\
(iv) from the reconstructed object's modulus a non-physical more intense spot in the center of the array had to be
filtered out. The occurrence of this spike indicates the presence of wrong zero frequency components in reciprocal
space, hence wrong zero RSP values. Fig. \ref{fig:stitching_filtering} shows the object's modulus before the removal of 
the high intensity spike located in the center. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/stitching_filtering.pdf}
    \caption{Predicted object's modulus before the filtering of the zero-frequency component, corresponding to noise 
    in reciprocal space. }
    \label{fig:stitching_filtering}
\end{figure}


\section{Patches: 3D case high strain}\label{chp:patches_strain}

For completeness, the same model has been trained on a dataset containing highly strained simulated diffraction patterns. 
Each BCDI pattern has been simulated following the procedure explained above in Sec. \ref{sec:dataset_creation3D} and
 \ref{sbsec:patches_creation} for a total amount of 50000 for both central and outer patches. The two CNN have been trained 
 for 50 epochs each and then tested on new simulated data. 

 For what concerns the first CNN trained on central patches it was possible to deduce a more difficult learning from the 
 loss curves. While in Fig.\ref{fig:loss_3D_lowstrain} the validation loss reaches 0.3 at the end of the training, for 
Fig.\ref{fig:loss_central_highstrain} it only drops to 0.6 with a significant divergence between training and validation 
starting from the 20th epoch. A higher loss value is nevertheless expected because of the more complex intensity-RSP mapping 
for the high strain cases, while the increasing gap between training and validation loss is a signature of poor generalization, 
early symptom of overfitting. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/loss_central_patch_highstrain.pdf}
    \caption{Training and validation loss trends for the training of \textit{Central-CNN} on central patches of highly strained BCDI 
    patterns. Higher loss values can be observed when comparing with the low strain case (Fig. \ref{fig:loss_3D_lowstrain}) and a 
    beginning of overfitting from the 20th epoch is also visible. }
    \label{fig:loss_central_highstrain}
\end{figure}

The results on test data show that the model does not always manage to correctly predict the RSP, especially when the 
iso-phase regions are not spherically symmetrical (Fig. \ref{fig:central_highstrain_RSP}c). It however succeeds to 
retrieve the correct RSP oscillations (Fig. \ref{fig:central_highstrain_RSP}a) inside the central fringes elongated by the
high strain. The reconstructions in real space, shown in Fig. \ref{fig:central_highstrain_obj}, confirm satisfactory 
results for the first column and poor results for the third column. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/central_patch_highstrain_RSP.pdf}
    \caption{Slices of the central patches. First row shows four different examples of simulated high-strain BCDI patterns cropped around the 
    center of the peak. Ground truth and predicted RSP are shown in second and third row respectively. The model manages 
    to estimate the correct RSP when this shows spherically symmetric iso-phase regions while struggles more for asymmetries. }
    \label{fig:central_highstrain_RSP}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/central_patch_highstrain_obj.pdf}
    \caption{Corresponding reconstructed objects. Except for case \textbf{c} the predicted RSP is good enough to well 
    estimate the size, shape and phase of the real space object. }
    \label{fig:central_highstrain_obj}
\end{figure}

Regarding the \textit{Outer-CNN} trained on outer patches of highly strained patterns instead, one can observe that 
the performance is not severely affected by the 
high-strain. Strong discrepancies between predicted and ground truth RSP are mostly present where there is no intensity signal, 
thus not relevant (see Fig.\ref{fig:outer_highstrain}). The good accuracy of the outer patches predictions suggests 
that the crucial and more challenging map to retrieve is the central patch. 

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\textwidth]{figures/Phasing/outer_patches_high_strain_RSP.pdf}
%     \caption{Examples of RSP prediction for outer patches cropped from simulated high-strain data. Similarly to 
%     the examples shown in Fig. \ref{fig:outerpatch_obj_lowstrain} for the low-strain case, the model yields relatively 
%     correct outputs. Worse predictions are observed where low intensity signal is recorded, therefore less important 
%     during the reconstruction.}
%     \label{fig:outer_highstrain}
% \end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/outer_patches_3D_highstrain.pdf}
    \caption{Examples of RSP prediction for outer patches cropped from simulated high-strain data. Similarly to 
        the examples shown in Fig. \ref{fig:outerpatch_obj_lowstrain} for the low-strain case, the model yields relatively 
        correct outputs. Worse predictions are observed where low intensity signal is recorded, therefore less important 
        during the reconstruction. }

    \label{fig:outer_highstrain}
\end{figure}
For completeness, the result of the full RSP stitching and the corresponding reconstructed object for 
the high-strain case is shown in Fig.\ref{fig:stitching_highstrain}. A simulated test data has been used for the ground truth comparison. It is clear that the 
stitching algorithm is performing poorly as observed for the low-strain case. However, the central RSP patch is fairly 
similar to the ground truth and therefore the low resolution reconstructed object shows roughly the correct shape and 
phase. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/stitching_high_strain_sim.pdf}
    \caption{Results of the stitching of RSP predicted outer patches for 3D simulated ``high-strain'' data (central 
    slice displayed). Compared to Fig.\ref{fig:stitching_exp_low} it is visible how the stitched RSP is more ``confused'' 
    outside the central patch. The less regular and symmetric fringes' spatial arrangement is not correctly recovered 
    during the stitching process. However, the discrete accuracy of the central patch allows for a close low-resolution 
    estimate of the object's modulus and phase. 
    }
    \label{fig:stitching_highstrain}
\end{figure}

\subsection{Discussion}
Although failed, this study on the prediction of the RSP in smaller patches has led to better understanding of the problem 
and nevertheless unveiled some interesting insights. For instance, it showed that the retrieval of the mapping between patches 
is possible with a CNN trained with the WCA loss function, untying the relationship with the real space object. 
Moreover, it emerged that the main difficulty of this approach is given by the stitching of the RSP patches into the full 
array. As mentioned above, the hypothesized reasons for this problem are (i) the fact that the model ``sees'' 
simulated ground truth RSP guesses during the training and predicted ones during inference, and (ii) the averaging 
of RSP predictions for overlapping voxels. In order to overcome these limitations other approaches were contemplated but 
never realized for lack of time. In particular, it was imagined a way to extract, analyze and stitch the patches inside the 
model into a sort of Recurrent Convolutional Neural Network (RCNN) that would keep track of the previous innermost shell
thanks to a dedicated convolutional Long-Short Term Memory (LSTM) \cite{shi2015convolutionallstmnetworkmachine}. By doing 
this the model would always be exposed to its own RSP predictions as initial guess for outer patches and the RSP average over
overlapping voxels could be replaced with a convolutional layer with non-linear activation function. While the attempts 
of setting up such model are not reported here, it is mentioned the idea as possible inspiration for future works. \\

To conclude, main finding of this study on patches is that it is crucial for a good object estimate to accurately predict 
the RSP in the vicinity of the center of the Bragg peak, and that the CNN model trained with the WCA can accomplish this task 
for highly strained patterns as well. It was therefore decided to invest the efforts into a regular CNN for the prediction 
of the RSP of 3D highly strained BCDI patterns with the intermediate size of $64\times64\times64$ pixels. 

\section{Final model design: 3D case high-strain for central patch only }\label{chp:3d_final_strain}

Following from the results and considerations relative to the patching approach it was decided to focus on the prediction 
of the central patch only. The size has been increased to $64\times64\times64$ pixels to include more features. Moreover, this 
is the size employed by most of the recent works presented in the beginning of the chapter \cite{yao_autophasenn_2022, wu_three-dimensional_2021, lim_convolutional_2021}. 
Similarly to the previous cases, the approach was to create a dataset to train the model with, in supervised manner with 
the WCA loss function. Only highly strained particles were considered, as it is for those that a succesful DL model can 
truly help the PR. 
The model here described and the results obtained on simulated and experimental 3D BCDI patterns can be found entirely 
in the paper "Phase Retrieval of Highly Strained Bragg Coherent Diffraction Patterns using Supervised Convolutional Neural Network" 
\cite{Masto2025} 


The architecture that was employed is an adaptation of the 3D U-Net employed for smaller patches Fig. \ref{fig:architecture_phasing}
As learned from the preliminary study on the 2D case, to better interpolate the diverse distribution of distorted BCDI patterns, 
the number of trainable parameters and samples in the training dataset was increased significantly. Additional encoder 
and corresponding decoder blocks were added for a total of 145 million parameters. Moreover, similarly to the model 
used for the gap inpainting, dilated convolutions were adopted in the first two encoder blocks in order to improve 
the receptive field of the model. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/Architecture-1.pdf}
    \caption{Schematic of the 3D U-Net model employed for the RSP prediction of highly strained patterns.}
    \label{fig:architecture_phasing}
\end{figure}

95000 simulated BCDI patterns have been created following the procedure described in Sec.\ref{sec:dataset_creation3D} 
on a cubic 64 pixel-sided grid for different particle's shape, strain distribution, oversampling conditions and noise levels. 
Another smaller dataset containing 4000 samples was created instead for testing the model. 

The model was trained with the WCA loss function for 60 epochs with a learning rate of $10^{-4}$. To speed up the process, 
the training has been conducted using two NVIDIA TeslaV100-SXM2-32GB GPUs using the MirroredStrategy feature for
synchronous training across multiple devices provided by the Tensorflow library. This measure allowed to reduce the training 
time from 2 hours per epoch to 30 mins. 

\subsection{Results: simulated data}\label{chp:phasing_results}

The model has been tested on simulated data. Figs.\ref{fig:RSP_paper_sim} - \ref{fig:obj_paper_sim}
illustrate some examples of comparison between ground truth and predictions for both the RSP and the reconstructed objects 
respectively. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/RSP_3Dsim.pdf}
    \caption{\textbf{\(\boldsymbol{a_0}\) - \(\boldsymbol{e_0}\))} Central slices of simulated input intensities from 
     the test dataset. \textbf{\(\boldsymbol{a_1}\) - \(\boldsymbol{e_1}\))} Corresponding ground truth RSP. 
     \textbf{\(\boldsymbol{a_2}\) - \(\boldsymbol{e_2}\))} Corresponding slices taken from the Deep Learning (DL)
     model prediction of the RSP. The RSP are displayed with an opacity filter 
     that highlights regions with diffracted intensity.}
    \label{fig:RSP_paper_sim}
\end{figure}

The model correctly predicts the RSP oscillations inside the fringes also when these have been distorted or merged into
a single continuous intensity stripe (Fig. \ref{fig:RSP_paper_sim} a-b) because of the high strain along the corresponding 
axis in real space.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/obj_3Dsim.pdf}
    \caption{Reconstructed objects from the diffraction patterns in Fig. \ref{fig:RSP_paper_sim}.  
    \textbf{a-e)} Central slices of both the modulus and phase for the ground truths and the DL reconstructions.}
    \label{fig:obj_paper_sim}
\end{figure}

Except for some noise and inhomogeneities in the objects' moduli, the reconstructions from the predicted RSP achieve 
good accuracy on simulated data, for different particle shapes and strain distributions. It is worth mentioning that 
the presence of noise affecting the objects' shapes is an effect of the loss function that is not computed in real space. 
The model is never directly shown that the real space object are compactly supported, as it comes as consequence of the  
correct RSP prediction. However, while small discrepancies of the RSP prediction result in noise on the object shape, 
an overall accurate RSP prediction ensures the retrieval of both the correct shape and phase of the object. This fact is 
of primary importance when considering the use of the DL prediction as starting point of iterative refinement with 
conventional algorithms. It is indeed easier to reach convergence from a low resolution noisy but accurate estimate of 
the object rather than a clean but inaccurate one. Moreover, in experimental conditions the shape can be known via other 
techniques (Scanning Electron Microscopy) while the strain field, derived from the retrieved object's phase, is the 
ultimate interesting result of the reconstructions.

\subsection{Results: experimental data}\label{chp:phasing_results}
Given the satisfactory results obtained on simulated data the DL model was also tested on experimental data. 
Two relevant examples of BCDI patterns collected at the ID01 beamline of the ESRF-EBS are considered here. 
The first pattern (Fig. \ref{fig:exp_RSP1} a) is given by a platinum nanoparticle on Yttria-stabilized zirconia (YSZ) 
(Particle 1) while the second (Fig. \ref{fig:exp_RSP2} b) is a dewetted platinum/palladium bilayer on a sapphire substrate
(Particle 2). 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/exp_data1.pdf}
    \caption{\textbf{a)} The three central slices of the 3D experimental diffraction pattern measured at ID01 from the Pt sample 
    (Particle 1). The images have been transformed in log-scale and renormalized between 0 and 1, ready to be processed 
    by the DL model. \textbf{b)} Corresponding slices of the DL model RSP predictions. }
    \label{fig:exp_RSP1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/exp_data_rec1.pdf}
    \caption{Corresponding three central slices of the reconstructed Particle 1 modulus (\textbf{a}) and phase 
    (\textbf{b}). Each slice has been cropped around the particle for better visualization.}
    \label{fig:exp_obj1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/exp_data2.pdf}
    \caption{\textbf{a)} Central slices of the 3D experimental diffraction pattern measured at ID01 from the Pt/Pd sample 
    (Particle 2). Note the shape of the diffraction pattern with broken centro-symmetry; signature of the high-strain. 
     \textbf{b)} Corresponding slices of the DL model RSP predictions.}
    \label{fig:exp_RSP2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/exp_data_rec2.pdf}
    \caption{Central slices of the reconstructed Particle 2, modulus (\textbf{a}) and phase  (\textbf{b})}
    \label{fig:exp_obj2}
\end{figure}

Fig. \ref{fig:exp_obj1} and \ref{fig:exp_obj2} show the reconstructed object obtained from the predicted RSP. Despite the 
low resolution and the presence of noise it is possible to recognize the shape of realistic particles (compact support 
and smooth phases).

In these cases of PR of experimental data it is impossible to establish a comparison with a ground truth because it 
is not available. However, the comparison has been made with the results obtained with standard iterative algorithms.
 
For this, 60 independent and randomly initialized runs have been launched with PyNX in order to reconstruct these datasets 
but no satisfactory results were obtained. Precisely, the recipe of 400 HIO + 1000 RAAR + 300 ER and different thresholds 
for the support estimation were used for each run (see Table \ref{table:pynx}). Moreover, the 3 best results according 
to the Mean-to-Max metric  \cite{Frisch2023CuAgCatalysts, Grimes2024CatalystStrain} were combined with mode decomposition 
\cite{favre-nicolin_free_2020} to improve the quality of the reconstruction. This last passage, through a Singular 
Value Decomposition (SVD) of the selected reconstructions, allows to estimate the reproducibility of the result by 
assessing the magnitude of the first eigenvalue (first mode). Values above 90\% typically imply good reproducibility, 
often associated to good convergence. 

However, in both cases the obtained reconstructions were not satisfactory as holes were present (Fig.\ref{fig:pynx_rec1}), 
or the support was excessively shrunk (Fig.\ref{fig:pynx_rec2}).\\

\begin{table}[H]

    \centering
    \resizebox{\linewidth}{!}{%
      \begin{tabular}{|l|l|}
        \hline
        \textbf{Parameter} & \textbf{Value} \\
        \hline 
        \texttt{obj}                                     & \texttt{autocorrelation} \\
        \texttt{support\_autocorrelation\_threshold}     & \texttt{(0.09, 0.11)} \\
        \texttt{recipe}                                  & \texttt{400 HIO + 1000 RAAR + 300 ER} \\
        \texttt{nb\_runs}                                & \texttt{60} \\
        \texttt{support\_update\_period}                 & \texttt{50} \\
        \texttt{support\_threshold}                      & \texttt{(0.15, 0.25)} \\
        % \texttt{smooth\_width}                           & \texttt{(2, 0.5, 600)} \\
        \texttt{update\_border\_n}                       & \texttt{3} \\
        % \texttt{smooth\_width\_begin}                    & \texttt{2} \\
        % \texttt{smooth\_width\_end}                      & \texttt{0.5} \\
        \texttt{post\_expand}                            & \texttt{(1,-2, 1)} \\

        % \texttt{update\_psf}                             & \texttt{20} \\
        % \texttt{psf}                                     & \texttt{'pseudo-voigt,0.5,0.1,10'} \\
        \hline
      \end{tabular}%
    }
    \caption{\textbf{PyNX parameter settings for standard PR.} Each of the 60 independent runs starts from an object 
    obtained from the inverse Fourier transform of the diffraction pattern (autocorrelation). Pixels above the autocorrelation's 
    maximum multiplied by a random threshold between (0.09 - 0.2) different for each run, are included in the support.
    In each run 400 cycles of HIO are followed by 1000 of RAAR and 300 of ER. The support is updated every 50 iterations 
    using the shrink-wrap algorithm \cite{Marchesini_shrinkwrap}. During any of these updates the pixels above the object's
    modulus maximum multiplied by a random value between (0.15 and 0.25) are included in the support. Moreover, only
    a shell of +/- 3 pixels around the outer border of the support are updated to avoid too large variations, and 
    a sequence of expansion, shrinking and re-expansion by (1,-2, 1) pixels respectively is evaluated at each update 
    step, in order to make the estimate more robust.}
    \label{table:pynx}
\end{table}


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/pynx_particle1_final.pdf}
    \caption{Combination of the 3 best reconstructions of Particle 1 out of 60 independent runs with PyNX (central slices). 
    Although the shape is guessed the presence of holes in the modulus and domains in the phase denotes a poor quality 
    of the result.}
    \label{fig:pynx_rec1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/modes_particle1-1.pdf}
    \caption{Phase of the three best reconstructions of Particle 1. Despite the similarity of the solutions, supported 
    by the large weight of the first SVD eigenvalue \cite{favre-nicolin_free_2020}, each of them and their combination 
    presents amplitude dips and phase domains not expected from the studied Pt sample. }
    \label{fig:pynx_rec1_modes}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/pynx_particle2_final.pdf}
    \caption{Combination of the 3 best reconstructions of Particle 2 out of 60 independent runs with PyNX (central slices). 
    The high strain induced by the substrate deceives the standard PR algorithm that excessively shrinks the object's support.}
    \label{fig:pynx_rec2}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/modes_particle2-2.pdf}
    \caption{Phase of the three best reconstructions of Particle 2. Low reproducibility of the result is highlighted by 
    the diversity of the reconstructions, supported by the relatively low first SVD eigenvalue. }
    \label{fig:pynx_rec2_modes}
\end{figure}


\section{Refinement with iterative algorithms}\label{chp:refinement}

It has been shown in Figs. \ref{fig:pynx_rec1} - \ref{fig:pynx_rec2} how the PR of highly-strained particles with 
conventional iterative algorithms and standard routines can be challenging. In order to achieve successful reconstructions, 
fine parameter tuning operated by expert users as well as large numbers of runs are often required. On the contrary, 
the DL approach, while fast at inference, is limited to low-resolution reconstructions and only generates outputs 
based on the approximation of the statistical distribution of simulated BCDI patterns. For this reason, the DL model 
for the phasing of experimental data needs be used as a preprocessing step that can provide 
a starting point for further iterative refinement. Few iterations of ER steps can be used to polish the DL estimate, simplifying 
the convergence, assumed that the DL estimate is located near the solution, in the search space. In this way, the DL model 
can play a crucial role in guiding the search toward the correct solution neighborhood, significantly reducing 
computational costs and overall wall-clock time.

At this stage it is worth mentioning that, since the DL model was trained on 64 pixel-size cubic volumes, it was necessary 
to crop and resize the experimental diffraction patterns. In fact, especially for large BCDI data, it is often needed a 
resizing or binning step before cropping, such that a sufficient amount of data is contained in the 64 pixel-size 
volume. This procedure requires special attention as for different 
cropping/resizing parameters the DL prediction varies. In particular, it is curious to notice that the cropping/padding 
operations in reciprocal space correspond to interpolations in real space and vice versa. For this reason the procedure 
illustrated in Fig.\ref{fig:resizing} had to be adopted to adapt the data to the DL format and then bring back 
the predicted object to the size relative to the original data one. This step is fundamental when the DL object is then 
used as starting point for further refinement with classical algorithms.  
Because of the large diversity of the region of interest (ROI) from one experimental BCDI pattern to the other, the manual 
intervention is for the moment needed to adjust the cropping and binning parameters such that the data fed into the DL 
model resembles the simulated data used for training.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/resizing.pdf}
    \caption{Manipulation of the datasets. Here the experimental BCDI pattern is firstly cropped around the COM of the Bragg peak, 
    cutting out parts of the signal. The data is then interpolated into a 64 pixel sided cubic grid and transformed in normalized 
    log-scale. The object obtained after the DL RSP prediction is then padded and resized back to the original shape such that it 
    can directly be plugged into an iterative algorithm for refinement. Note that the size of the objects are already 
    shown in boxes of half the size with respect to the corresponding diffraction patterns.}
    \label{fig:resizing}
\end{figure}

If the DL predicted object is a good low-resolution guess of the solution, few steps of ER are usually enough to reach 
convergence. In this specific case 300 iterations of ER were performed and, since the estimated DL object is assumed 
to be close to the solution, only the pixels at the border the support were allowed 
to be updated (see Table \ref{table:DLpynx}). Figures \ref{fig:dlpynx_rec1} and \ref{fig:dlpynx_rec2} show the results 
obtained after refinement. Theoretically, one could tune the number of pixels updated at the border of the support depending 
on the resolution mismatch between the DL data ($64\times64\times64$) and the full BCDI size. The larger the size difference 
the more the pixels at the border and vice-versa. However, it was found that a single pixel at the time is often the 
best choice. 

\begin{table}[H] 

    \centering
    % \resizebox{\linewidth}{!}
    {%
      \begin{tabular}{|l|l|}
        \hline
        \textbf{Parameter} & \textbf{Value} \\
        \hline 
        \texttt{obj}                                     & \texttt{DL\_obj} \\
        \texttt{recipe}                                  & \texttt{300 ER} \\
        \texttt{nb\_runs}                                & \texttt{1} \\
        \texttt{support\_threshold}                      & \texttt{0.3} \\
        \texttt{support\_update\_period}                 & \texttt{50} \\
        % \texttt{smooth\_width}                           & \texttt{(2, 0.5, 600)} \\
        \texttt{update\_border\_n}                       & \texttt{1} \\
        \texttt{post\_expand}                            & \texttt{(1,-1)} \\
        % \texttt{smooth\_width\_begin}                    & \texttt{2} \\
        % \texttt{smooth\_width\_end}                      & \texttt{0.5} \\
        % \texttt{update\_psf}                             & \texttt{20} \\
        % \texttt{psf}                                     & \texttt{'pseudo-voigt,0.5,0.1,10'} \\
        \hline
      \end{tabular}%
    } 
    \caption{\textbf{PyNX parameter settings for the refinement after the DL prediction} A single run of 300 ER iterations 
    using a support threshold of 0.3 with respect to the maximum of the object's modulus for the support update. This 
    last is performed with the shrink-wrap algorithm, every 30 iterations. At this stage, the only pixels affected 
    by the support updated lie within +/- 1 pixels around the outer border of the support. Moreover, the support is 
    also shrunk and expanded by 1 pixel at each update step. }
    \label{table:DLpynx}
  \end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/exp_data_dlpynx1.pdf}
    \caption{Central slices of Particle 1 after 300 cycles of ER for refinement of the DL initial estimate. }
    \label{fig:dlpynx_rec1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/exp_data_dlpynx2.pdf}
    \caption{Central slices of Particle 2 after 300 cycles of ER for refinement of the DL initial estimate.}
    \label{fig:dlpynx_rec2}
\end{figure}

The improved homogeneity of the modulus and the more physical shape of a Winterbottom particle are a clear sign of 
better reconstructions for both cases. One can observe that the shape and phase of the final reconstruction are not 
remarkably different from the DL estimates. This proves that the DL model is able to generalize correctly to experimental 
data as well and that the following ER steps are a good choice for refinement. It is worth noticing that launching several 
independent runs with the same initial DL guess would not change the outcome as ER converges to the local minimum by 
projecting each estimate on the constraint sets, therefore never ``exploring'' other neighborhoods of the solution space. \\

While for Particle 1 it is obvious that the DL + PyNX solution is better than the PyNX one because of the absence of ``holes''
in the modulus, for Particle 2 it can be harder to believe that the DL + PyNX approach actually yields an improved solution. 

Although greater homogeneity of the electron density, as quantified by the Mean-to-max metric, often suffices to 
regard one solution as superior to another, rigorous assessment requires validation against experimental data 
through evaluation of the discrepancy between observed and calculated intensities.
Here, Fig. \ref{fig:dlpynx_proj2} shows that the calculated intensity obtained from the DL + PyNX 
is more accurate than the one obtained form the PyNX - only solution for both Particles 1 and 2. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/proj_particle2.pdf}
    \caption{Projection along the three axis of the absolute difference between observed and calculated diffraction patterns 
    relative to Particle 2. 
    The mismatch (MAE) of the one calculated from the particle reconstructed using PyNX only, is higher than the one calculated 
    from the particle obtained with the DL + ER method. This proves that the latter is the correct solution. }
    \label{fig:dlpynx_proj2}
\end{figure}

Beside the higher quality of the results it is worth mentioning the significant reduction of the wall-clock time. 
Table \ref{tab:times} summarizes the time taken for the 3 different methods. A 40X to 50X speed-up is recorded for 
these dataset sizes and the time saving increases for larger ones.

\begin{table}[H]
    \centering
    \begin{tabular}{ccc}
         & \textbf{Particle 1} & \textbf{Particle 2} \\
        \textbf{Data size} & (118,180,230) & (110,160,200) \\
        \textbf{DL inference} & 3.04 s & 2.21 s \\
        \textbf{PyNX: 60 runs} & 227.41 s & 123.55 s \\
        \textbf{DL + PyNX: 1 run} &  4.73 s & 3.08 s \\
    \end{tabular}
    \caption{Computation times for the three different methods. DL RSP prediction and inverse FT are considered for 
    DL time. PyNX - only time includes 60 runs of 400 HIO + 1000 RAAR + 300 ER performed in parallel by PyNX. The best 
    reconstructions selection and following mode decomposition is not considered. DL + PyNX time includes 
    the full DL time plus the reshaping of the initial guess to the original size and a single run of 300 ER. }
    \label{tab:times}
\end{table}

For better comparison, Particle 2 has been reconstructed using a genetic PR approach. Similarly to the procedure illustrated 
in \cite{Ulvestad2017}, from 15 randomly initialized guessed the following iterative chain was applied:
\begin{itemize}
    \item 400 iterations of HIO
    \item 300 iterations of RAAR
    \item 100 iterations of ER
\end{itemize}

One reconstruction was selected from the population based on a sharpness criterion, defined as the sum over all voxels 
of the absolute value of the reconstruction raised to the fourth power (see \cite{Ulvestad2017}).
    
The modulus of this selected reconstruction was then used as a seed for the next 10 generations of the base chain, 
with the phase kept unchanged. This update followed Wilkin's approach \cite{WilkinGenetic_2021}:

\begin{equation}
\rho = \sqrt{\rho \times \rho_{\text{best}}}
\end{equation}

where $\rho$ is the modulus of an individual in the population being updated, and $\rho_{\text{best}}$ is the 
modulus with the lowest sharpness from the previous generation.
The results are shown in Fig. \ref{fig:clement_genetic}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/clement_genetic.pdf}
    \caption{Central slices of Particle 2 reconstruced with genetic PR. Although better than Fig. \ref{fig:pynx_rec2}
    the result is far from what obtained with the DL-aided PR. }
    \label{fig:clement_genetic}
\end{figure}

At this point, it is evident that the diffraction pattern of Particle 2 cannot be reliably reconstructed without a 
DL-based initial estimate. This difficulty is commonly associated with the presence of local minima, which hinder 
the search for the correct solution and can trap iterative algorithms. An interesting observation is that this behavior 
can be linked to the phase range of the reconstructed object. Specifically, if the phase values are confined within 
a single phase wrap, i.e. the interval $[0,2\pi)$, the phase difference between any two voxels is uniquely defined 
within that domain. In contrast, objects with broader phase ranges typically extend over multiple phase wraps, in which 
case the phase difference between voxels is not uniquely determined in $[0,2\pi)$. 
It follows that the larger the range of the unwrapped phase, the 
higher the ``population of local minima'', hence the more challenging the PR. Fig. \ref{fig:obj_ortho} shows the 
histogram of the phase range exending over the equivalent of two full wraps of Particle 2 (\textbf{a}) as well as 
with the strain distribution \textbf{b} on the 3D rendering of the orthogonalized reconstruction. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/ortho_particle-3.pdf}
    \caption{\textbf{Particle 2.} \textbf{a} Histogram representing the range of values of the reconstructed object's 
    phase. Such broad range is typically referred to as ``strong-phase'' or ``high-strain'' and implies challenging PR. 
    \textbf{b} 3D rendering of the particle's strain distribution after orthogonalization. The morphology confirms the 
    Winterbottom shape expected from Pd/Pt on crystalline substrate. }
    \label{fig:obj_ortho}
\end{figure}


\newpage


\section{Performance assessment}\label{chp:performance}

In this paragraph the results of the DL model presented above, tested against different strain configurations and 
magnitudes are discussed. 
The scope of the study is to assess the model's performance for these different cases, to evaluate when it works best 
and ultimately to estimate the gain in accuracy when coupled with ER refinement. 
The first test was prepared by simulating a Winterbottom shaped particle, similar to the ones used for the training set,
with an applied phase built with two Gaussian functions with two different and increasing amplitude ranges. The 
corresponding BCDI pattern has been simulated for each case, keeping the same oversampling ratio and noise level across 
the simulations. For each calculated diffraction pattern the RSP has been predicted using the DL model and corresponding 
objects have been obtained with inverse FT. At this point the accuracy of the prediction was calculated with using the 
formula in \ref{eq:accuracy_phasing} and the results are shown in Fig.\ref{fig:gauss}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/performance_gauss.pdf}
    \caption{Evolution of the BCDI pattern and corresponding RSP and object for higher strain calculated with the sum of 
    two Gaussian functions of increasing amplitude. Third and fifth row show the DL predicted RSP and the reconstructed 
    object's phase respectively. As expected the prediction worsens for high phase ranges but it overall maintains a similarity 
    with the ground truth. One could notice the presence of aliasing due to the use of the Fourier transform and large
    oversampling. Although not observable in experimental conditions, this artifact does not affect severely the analysis 
    and the reconstructions.}
    \label{fig:gauss}
\end{figure}

As expected the accuracy drops as the phase range increases.
It is interesting to notice that despite the worsening of the prediction with the increasing strain the object's phase 
structure inside the support resembles the ground truth phase. This detail is fundamental for the ER refinement as the 
initial guess, notwithstanding the inhomogeneous and ``shattered'' support, represents a good estimate of the solution. 
Objects with similar cleaner supports of incorrect shapes and phases are much worse starting points for iterative refinement 
since they are far from the solution. \\

The same procedure has been repeated for a different strain configuration, this time constructed with two cosine functions 
with increasing amplitude. The phase distribution induced by these functions is rather different, more in the spatial 
structure than the amplitude. Fig. \ref{fig:cosine} shows that the model accuracy is significantly 
poorer than the previous case. It is worth recalling that the training set was composed of equal amounts of particles 
simulated with Gaussian and cosine phase profiles, meaning that the lower accuracy scores are not due to some possible 
imbalance of the training. The phase range inside the particles of Fig. \ref{fig:cosine}, that can be estimated visually 
by counting the number of phase wraps, is also in the same order of magnitude as the one shown Fig. \ref{fig:gauss}. 
This further suggests that the poorer performance observed in Fig. \ref{fig:cosine} arises from other factors.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/performance_cosine.pdf}
    \caption{Evolution of the BCDI pattern and corresponding RSP and object for higher strain calculated with the sum of 
    two cosine functions of increasing amplitude. Third and fifth row show the DL predicted RSP and the reconstructed 
    object's phase respectively. The model rapidly struggles with this type of phase field.}
    \label{fig:cosine}
\end{figure}

Similarly, the model seems to struggle more for phase fields simulated with using Gaussian correlated random field (same as
in \ref{sec:dataset_creation3D}) than for the first case. Again the reason doesn't seem to be related to the different 
phase range nor population imbalance in the training set (Fig. \ref{fig:random1})

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/performance_random.pdf}
    \caption{Evolution of the BCDI pattern and corresponding RSP and object for higher strain applied by using a Gaussian
    correlated random field of increasing amplitude. Third and fifth row show the DL predicted RSP and the reconstructed 
    object's phase respectively. }
    \label{fig:random1}
\end{figure}

To better assess the performance of the model on the three different cases, the same procedure has been repeated on a 
bigger and more diverse population. Namely, 20 randomly shaped particles have been simulated and for each of them an 
increasing strain field has been applied in 10 different steps, for the three types of distributions. The results of 
the model are shown in Fig. \ref{fig:compa_all}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/plot_acc_comp_allFINAL.pdf}
    \caption{Mean accuracy of the DL model for different phase ranges and types. The phase field simulated with two Gaussian 
    functions yield RSPs that are better predicted by the DL model than the ones obtained with two cosine functions or 
    random Gaussian fields. }
    \label{fig:compa_all}
\end{figure}

Another study that has been conducted aimed at estimating the accuracy gain of the DL model coupled with ER refinement 
compared to the sole iterative algorithm. In this case a single Winterbottom particle has been selected for simplicity, 
and an increasing phase field was applied in 50 steps for the three different types. At this point the accuracy scores 
have been calculated for the DL model only first. Separately, a single run of 400 HIO + 1000 RAAR + 300 ER has been 
performed using PyNX for each of the 50 diffraction patterns. At last, the DL model predicted objects have been used
as starting point for 300 ER refinement, always with PyNX. In all cases, it was made sure that the solution found by either 
the DL or the iterative algorithm wasn't the twin of the ground truth. If so, the object was flipped before calculating 
the accuracy. 

The analysis has been repeated for the three different phase fields used in the above cases and the results are 
shown in Figs. \ref{fig:compare_gauss}, \ref{fig:compare_cosine}, \ref{fig:compare_random}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/plot_acc_comparison_gauss_winter.pdf}
    \caption{Comparison of the accuracy scores of the DL model prediction, iterative PR and DL + ER approaches for objects with 
    phase fields simulated with two Gaussian functions with increasing amplitude. The DL + ER reconstructions always achieve 
    higher accuracies than the other methods, also in those cases in which the DL ones are low. }
    \label{fig:compare_gauss}
\end{figure}

While for lower phase ranges the DL prediction is worse than the two other methods, the main result of Fig.\ref{fig:compare_gauss} 
is that in all cases the accuracy of the DL + ER method is always superior, meaning that the DL model can significantly 
aid the PR process also when the first estimate is not very close to the solution (down $50\%$ accuracy in the figure.).

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/plot_acc_comparison_cosine_winter.pdf}
    \caption{Comparison of the accuracy scores of the DL model prediction, iterative PR and DL + ER approaches for objects with 
    phase fields simulated with two cosine functions with increasing amplitude. }
    \label{fig:compare_cosine}
\end{figure}

Similarly, the DL model proves to be significantly helpful at providing a good initial guess for the ER refinement when 
the object's phase is simulated with cosine functions. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/plot_acc_comparison_random_winter.pdf}
    \caption{Comparison of the accuracy scores of the DL model prediction, iterative PR and DL + ER approaches for objects with 
    phase simulated with a Gaussian correlated random field with increasing amplitude. The combination of DL + ER yields better 
    results up to a phase range value of 10 radians. For higher values the poor DL prediction does not provide a good enough 
    estimate of the solution thus it does not help the convergence of the ER method. }
    \label{fig:compare_random}
\end{figure}

In this case as well, the DL model, although when used alone yields relatively low accuracy scores, improves the quality of the 
solution when coupled with some ER cycles for refinement. At the same time it is interesting to notice that when the 
accuracy of the DL prediction is too poor - most likely when, as in Fig.\ref{fig:random1}, the object's phase differs too 
much from the ground truth one - the ER cycles do not improve the result as the initial estimate is wrong. 

Finally the reasons behind the imbalance of accuracy scores for different phase 
fields are investigated on a qualitative level. From the visual assessment of the DL prediction one can observe that 
the model performs best when the RSP is characterized by ``iso-phase'' lines with spherical symmetry. This effect was 
already observed in the 2D case in which the WCA loss managed to overcome this limitation. In the 3D case this effect is 
more pronounced and the model still struggles to break this structural symmetry despite the large number of trainable parameters 
and training samples. A possible explanation is given by the symmetry of the diffraction pattern used in input. The averaged 
spatial intensity distribution of the ensemble of training samples possess a spherical symmetry, decaying with a power 
law radially. As a consequence, the model replicates a similar distribution in the predicted RSP. This result is clearer 
when the direct output of the model, in the form of an unwrapped RSP, is inspected. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/non_centrosymmetric_study_3d_random_LS0.pdf}
    \caption{Example of DL prediction for a low strain randomly shaped particle. The non-centrosymmetriciy of the object's 
    shape and the small strain produce a symmetric diffraction pattern with a RSP that when unwrapped possesses a non-spherical 
    symmetry. This deceives the model that wrongly predicts a symmetric RSP, resulting in an incorrect reconstructed object. }
    \label{fig:non_centrosymm_LS}
\end{figure}

When comparing the unwrapped phases one can observe that while the ground truth, along the main streaks, grows monotonically 
from one to the other end, the predicted one, which is the direct output of the model, grows isotropically along all radial 
directions. The consequence is a wrong retrieved object as visible in Fig.\ref{fig:non_centrosymm_LS}. 
It is worth noticing that the example reported in the figure is a low-strain case, meaning that the main obstacle for 
the model is given by the structural symmetry of the RSP rather than the strain. 
To support this hypothesis, a stronger phase obtained with two Gaussian functions has been applied to the same object and 
the same test has been conducted. The results illustrated in Fig. \ref{fig:non_centrosymm_HS} show that the RSP symmetry 
has changed to a more ``spherical'' one thanks to the strain. Therefore, the predicted RSP is more accurate as well as 
the reconstructed object's shape. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/non_centrosymmetric_study_3d_random_HS0.pdf}
    \caption{A stronger phase has been applied to the same object in Fig. \ref{fig:non_centrosymm_LS}. The strain field 
    produces a more symmetric RSP that is correctly retrieved by the DL model, thus a better object is retrieved despite the 
    higher strain. }
    \label{fig:non_centrosymm_HS}
\end{figure}

\section{Other model test}\label{sec:other_model}

In this last section I would like to assess the real advantage of predicting the RSP rather than the complex real space object. 
In order to conduct this experiment, the most recent model in the literature of Deep Learning for BCDI Phase Retrieval was 
considered. In particular, the model presented by Yu and coauthors in \cite{yu_ultrafast_2024}, that makes use of complex 
convolutional layers seem to be a good candidate for the testing of our DL model.
The 2D model that was presented in the paper was transformed into a similar one for 3D data, for a total 
amount of 36 million trainable parameters. 

The model has been trained for the same amount of epochs (60) on the same training dataset used for our DL model. Model structure 
and loss function were adopted like those presented in the paper and the tests have been performed on the same data shown in Fig. \ref{fig:obj_paper_sim}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/sim_obj_Ian.pdf}
    \caption{Predicted objects obtained by the 3D adaptation of the 2D Complex U-Net presented in \cite{yu_ultrafast_2024}.
    The model returns a complex tensor that represents the real space solution. Although much less noisy than our predictions, 
    the reconstructed objects possess incorrect shapes and phases. }
    \label{fig:Ian_objs}
\end{figure}

The model returns objects that have a better resolution and less noise levels because of the loss function was calculated in 
real space. However, the shape and phase of the results are not correct, thus unusable for ER refinement because they 
are misleading initial guesses. 
In fact, the PR of the diffraction pattern of Particle 1 (Fig. \ref{fig:exp_RSP1}) has been 
inferred with this model (see Fig. \ref{fig:ian_rec1}) and the result used as starting point for further refinement 
using PyNX and the parameters of Table \ref{table:DLpynx}. The model prediction possesses clean and uniform modulus, 
and a support which is almost correct. However the phase is quite different from what obtained in \ref{fig:dlpynx_rec1}. 

For this reason, the predicted object does not represent overall a close enough estimate for ER to converge to the 
solution. The result instead shows inhomogeneous modulus with amplitude dips and domains in the phase, therefore far 
from the correct solution.  
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/ian_dl_particle1.pdf}
    \caption{Three central slices of the prediction of Particle 1 using the DL model presented in \cite{yu_ultrafast_2024} 
    adopted for the 3D case and trained on highly strained BCDI data. The retrieved object is clean and homogeneous but the 
    phase has been revealed to be incorrect. }
    \label{fig:Ian_rec1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/Phasing/ian_pynx_particle1.pdf}
    \caption{PR of Particle 1 obtained with PyNX refinement following Table \ref{table:DLpynx} of the DL prediction in 
    Fig. \ref{fig:Ian_rec1}. The results show that the convergence to the correct solution was not achieved because the DL
    estimate is too far from it.}
    \label{fig:Ian_pynx}
\end{figure}

This simple example shows that the prediction of the RSP suites better the PR of BCDI patterns with convolutional 
neural networks, especially for highly strained particles. 

\section{Conclusion}

To conclude this journey through the developments, results and interpretations of the DL model for the BCDI RSP prediction 
the main points can be summarized as follows.
\begin{itemize}

    \item A novel approach for the DL-based PR of BCDI patterns, focused on the prediction of the RSP has been investigated. 
    The method proved to be advantageous since (i) it entails the inference of a single array rather than two coupled ones, 
    (ii) it exploits the exact calculation of the IFT, directly using the diffraction pattern in the transform, rather than 
    a learned map from the diffraction pattern and the solution in real space. (iii) it takes advantage of the similarity between 
    the input and the output variables, making the best use of the skip connections of the U-Net architecture.
    The superiority of this approach over the more conventional one found in literature has been demonstrated as discussed in 
    \ref{sec:other_model}. 

    \item The training of the DL model for the prediction of the RSP can be optimized with the use of the custom WCA loss function, 
    designed specifically for the handling of complex phases. This loss function naturally resolves the wrap, the sign 
    and the offset symmetries inherent to the lost phase problem. Adopting the WCA loss not only improves the results but also 
    speeds up the training as it takes shorter time than a loss computed in real space. Moreover, the use of the WCA avoids 
    any reference to the real space object during the training, opening the door for a patching approach, where the RSP is 
    predicted from a sub-volume of diffracted intensity. 

    \item Such a tailored model is trained in supervised fashion on simulated data only. This can be at first a limiting factor 
    as it requires the simulation of big and diverse datasets. However, as the BCDI technique is restricted to single 
    crystals, the population of shapes, though being potentially infinite, is physically confined by nature. It is thus easier for 
    the DL model to generalize for new particle shapes. 

    \item The DL model trained as presented is able to perform on highly-strained particles as well, showing promising
    results in 2D and 3D. It has proven to be successful inverting experimental data that happened to be extremely challenging with 
    conventional iterative methods only. This achievement marks a milestone for the BCDI community, upgrading the DL studies
    on BCDI PR from a \textit{``proof of concept''} level to an actual \textit{practical use} in the analysis of experimental data. 

    \item The DL model proved to be compatible with iterative refinement using conventional algorithms, contributing to 
    a faster and more robust pipeline for PR. The computational time and costs typically devoted to the search of the 
    global minimum with regular algorithms is now significantly reduced by the DL estimate. This can potentially broaden 
    the field of application of BCDI to many more samples in which high-strain is involved as cause or consequence of physically 
    relevant mechanism.

    \item The DL model exhibits certain shortcomings attributable to the intrinsic symmetry of RSP. It in fact ``prefers''
    RSPs with a spherically symmetric distribution and the major hypothesis for this fact is that such symmetry is typical 
    of the intensity distribution of the average diffraction pattern. Hence, the increased difficulty of the model to escape this
    bias for 3D data as compared to 2D data. Additionally, it is curious to notice that this symmetry seems to be the strongest 
    limitation for the DL model, lying above the sign symmetry and the high strain, which are both resolved when in many cases. 
    Future investigations that elucidate the root causes of this phenomenon may yield improved performance on more heterogeneous 
    data sets. 
    
    \item At last, the substrate-induced strain imposed on Winterbottom particles often manifests a symmetry that is faithfully 
    reproduced by the pair of Gaussian functions used to generate the phase of the training ensembles. This symmetry gives rise to 
    a spherically symmetric RSP, which the DL model, by design, generally predicts with high accuracy. 

  \end{itemize}
