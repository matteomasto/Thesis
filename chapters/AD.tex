% In this last chapter an approach to the BCDI Phase Retrieval based on Automatic Differentiation will be discussed.
% It started from the necessity to 
% Unlke the DL model discussed above this method is iterative and

In this chapter a different approach to the BCDI phase retrieval will be presented. It originated from the need to resolve 
those cases in which neither standard alternating algorithms, nor the DL assisted PR can succeed to converge to a satisfactory 
reconstruction. The developed approach differs from the alternating projections algorithms classically used for 
the Fourier PR, as it is formulated as minimization problem solved with gradient descent (GD). The gradients however are computed 
through the efficient automatic differentiation (AD) enabled by graph-based differentiable programming packages like Tensorflow and 
PyTorch, accelerated on GPU. For this reason one could see the AD approach as unsupervised machine learning on a single training 
dataset.\\ 
The GD - based optimization is fundamentally different from fixed point alternating projections. Here one could qualitatively say 
that if the latter switches between real and reciprocal space applying constraints in both, the former initializes a 
complex object and updates at each cycle its modulus and phase using the gradients, with respect to them, of the differences 
between the observed and calculated diffracted intensities. In this way, the knowledge on the particle can be implemented 
by initializing the object with some physical constraints or adding regularization terms that will drive the updates 
towards more reasonable solutions. \\

After mentioning the most relevant literature on AD, and more generally GD-based, phase retrieval for CDI, 
we will present our formulation of the problem and the results obtained on simulated and experimental BCDI patterns. 

\section{State of the Art}
\section{Model implementation}
In an AD-driven optimization problem some trainable parameters are initialized. In the first basic formulation these 
trainable parameters can be the values of the voxels corresponding to the modulus $m$ and the phase $\varphi$ of the complex objects 
that represents the solution of the PR problem. All of these voxels contribute to the creation of a simulated 
diffracted intensity pattern via the forward model $I_{calc} = |\mathcal{F}\left\{ me^{i\varphi} \right\}|^2$. Subsequently, 
the gradients of a metric (loss function) that estimates the distance between the observed BCDI pattern $I_{obs}$ and $I_{calc}$ are calculated 
with respect to each of the trainable variables with automatic differentiation. At this point the value of each of these voxels is 
updated using a chosen optimizer (SGD, ADAM, etc.) and a given learning rate. The Tensorflow library allows for an easy 
implementation of the trainable variables and loss function and handles gradient operations with predefined methods. It is therefore 
straightforward to run the optimization as it follows the same structure of a deep learning model, with less trainable parameters and 
for a single data. 

However, such simple formulation of the complex object as mere real-valued variables is not optimal for a non-linear and non-convex 
inverse problem such the Fourier phase retrieval. In fact, many non-physical modulus-phase configuration could yield a 
$I_{calc}$ that is close to  $I_{obs}$. The presence of these local minima is the reason why, in conventional PR, algorithms 
like hybrid input-output, capable of escaping them, are employed. 
Moreover, it was shown by Marchesini in \cite{marchesini_unified_2007}
that steepest GD and even more sophisticated conjugate GD are more prone to get stuck in local 
minima, reason why they are not commonly utilized for Fourier PR. However, the active research field of machine learning has 
brought important advancements in the formulation of efficient and robust optimizers based on stochastic gradient descent with 
powerful features like Nesterov or adaptive momentum (ADAM \cite{ADAM}). These GD techniques are more robust to local minima, 
since the gradient is computed on mini-batches of trainable variables rather all of them (stochastic rather than classical steepest GD), 
and converge faster thanks to the ``memory'' of previous steps. Additionally, they are often wrapped into handy classes, ready to use, 
in Tensorflow and Pytorch libraries.  

However, to facilitate the convergence the formulation of the complex object to be optimized has embedded some physical considerations that 
helped to restrict the solution space. First of all, both support and phase built on a 3D grid occupying half the volume of the 
input BCDI data to account for the oversampling ratio which has to be at least 2 in all directions to ensure invertibility.  
Additionally, other constraints specifically designed for the object shape and phase were considered. 

\subsection{Object's shape}

The formulation of the object's shape has started considering the typical crystalline samples that are studied with the BCDI technique
and the requirements the modulus of the reconstructed object need to fulfill to be considered a ``good solution''. 
Usually, successful reconstruction show a \textit{homogeneous} modulus, sometimes quantitatively assessed through the 
mean-to-max metric \cite{Frisch2023CuAgCatalysts} , \cite{Grimes2024CatalystStrain}, as in standard BCDI the form factor is 
approximated uniform across all the scattering sites. Enforcing a homogeneous modulus by construction limits the search space 
and helps the convergence to the solution. 
It follows that parametrizing the \textit{surface} of the support, and setting to 1 the inside, is much more advantageous than optimizing 
the full 3D volume. This approach, already proposed by Scheinker and Pokharel in \cite{scheinker_adaptive_2020}, 
also significantly reduces the number of variables to optimize.

An additional consideration is that the probed samples are crystalline, thus often \textit{faceted} and \textit{convex}. 
Therefore, one could simplify even more the construction of the object shape by building a certain amount of planes in the 
3D space and obtain the support from the volume that lies inside the intersections of all them. This would remove the possibility 
to have spikes or rough surfaces that might satisfy some local minimum but wouldn't represent a crystal. Moreover, with this 
representation the number of trainable variables would be further reduced. 

According to this scheme the relevant parameters to be optimized are the angles $\theta$ and $ \varphi$ of the spherical 
coordinates and the length $d$ of a given number $N$ of the so-called \textit{half-spaces}. 
More formally, the normals $n_i$ for each of the $N$ half-spaces are defined with a pair of ($\theta , \varphi$) that  its
orientation in space (Eq. \ref{eq:normal_vector}). Subsequently, only the intersection of those $(x,y,z)$ coordinates for which the dot product with 
each $n_i$ is smaller than the length $d_i$ is considered as support (Eq. \ref{eq:convex_hull}).


\begin{equation}
    \mathbf n_i \;=\;
    \begin{pmatrix}
    \sin\varphi_i\cos\theta_i \\[6pt]
    \sin\varphi_i\sin\theta_i \\[6pt]
    \cos\varphi_i
    \end{pmatrix},
    \label{eq:normal_vector}
\end{equation}
    
\begin{equation}
    \mathcal S \;=\;
    \bigcap_{i=1}^N
    \Bigl\{\mathbf x=(x,y,z)\in\mathbb R^3 : 
    \mathbf n_i\cdot\mathbf x \le d_i\Bigr\},
    \label{eq:convex_hull}
\end{equation}

A schematic representation of this construction is provided by Fig. \ref{fig:support_construction}.
 
With this approach the user needs to provide a number of half-spaces as hyperpareter meaning that a sort of prior knowledge 
on the sample can be leveraged in these regards as well. However, this number doesn't have to be precisely the number of 
facets expected. In fact, a large $N$ is often advised for unknown sample shape such that even roundish objects can be 
retrieved. In case of well faceted samples the large $N$ is a minor problem as many $n_i$ will be automatically aligned to the 
same $(\theta_i, \varphi_i, d_i)$ at the cost of some more trainable parameters. 

The first drawback of this convex-hull parametrization is that concave objects can't be retrieved. However, these cases 
are much less frequent in typical BCDI experiment. The second limitation is that this formulation is incapable of modeling 
defects that would zero the contribution of the object's modulus to the diffraction pattern \cite{favre-nicolin_analysis_2010}. 
A correct BCDI reconstruction of particles affected by this type of defects presents ``holes'' inside the hull in correspondence of the 
defect. However, the current model cannot address this type of features as the support is by construction fully homogeneous 
inside the borders. Further developments of the algorithm could indeed aim at a more complete formulation of the construction 
of the object modulus. 

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{figures/AD/AD.pdf}
    \caption{Construction of the convex hull with half-spaces expressed with spherical coordinates }
    \label{fig:support_construction}
\end{figure}

The last important consideration of this parametrization is that the support $\mathcal S$ is sharply divided into a binary 
variable (1 inside and 0 outside) thus leading to differentiability problems. In fact, in such a way the gradients, essential for the 
support update, are not defined. For this reason $\mathcal S$ is first passed through a sigmoid function controlled by a
hyperparameter $\epsilon$ responsible for the smoothening of the support borders. This measure can also be seen as a control of the 
resolution of the object. Additionally, a mildly steep sigmoid in the early stage of the optimization can function help retrieving 
a low resolution estimate of the support, that can be further refined adjusting the $\epsilon$ parameter. \\

\subsection{Object's phase}
The parametrization of the object's phase is more challenging. From a qualitative point of view, the prior knowledge 
that can be exploited for a tailored implementation, is limited to the awareness that a physically meaningful atomic displacement 
field cannot have ``too many'' sharp variations. This observation is translated into code by smooth functions parametrization or total 
variation (TV) regularization of the object's phase. While the former would enforce smoothness by construction the latter 
would operate adding a penalty to the data-fidelity term of the loss function for non-smooth solutions. Both approaches have been 
explored and are here reported. \\

Forcing a scalar field defined on an  $L\times H\times W$ grid to exhibit smooth behavior is equivalent to seeking a 
sparse representation of that field—that is, to concentrating its essential information into far fewer degrees of freedom 
than the original $L\times H\times W$ samples. 
Concretely, one looks for a change of basis in which the field can be written as a linear combination of a hierarchy 
of modes or atoms, ordered by “importance.” In a Fourier or wavelet expansion, for instance, the expansion coefficients 
are naturally sorted from largest (low‑frequency or coarse‑scale modes) to smallest (high‑frequency or fine‑scale modes). 
Retaining only the largest coefficients both compresses the data and removes rapid oscillations, yielding an inherently 
smoother reconstruction. Equivalently, in the matrix case a Singular Value Decomposition (SVD) identifies an orthonormal 
basis in which only a few singular values are nonzero; by truncating to the top singular values one obtains a low‑rank 
— and thus smoother—approximation \cite{golub1996matrix}. For higher dimensional data, this same principle underlies 
higher-order generalizations of the SVD—Tucker/HOSVD, CP, Tensor-Train, and T-SVD—each of which orders multilinear 
“modes” by their singular-value (or eigenvalue) strength, and truncating to a small subset produces both compression 
and smoothness \cite{Kolda_TT}. 

In this case the Tucker decomposition was chosen, among the several possible methods, for its simplicity of implementation 
with the Tensorflow library and for the suitability for moderately low dimensions \cite{Oseledets_TT}. 
For a 3D tensor the Tucker decomposition is done as follows: 

Considering \(\mathcal{\varphi} \in \mathbb{R}^{L \times H \times W}\) the 3D object's phase. The Tucker decomposition expresses \(\mathcal{\varphi}\) as:
\[
\mathcal{\varphi} = \mathcal{G} \times_1 U^{(1)} \times_2 U^{(2)} \times_3 U^{(3)},
\]
where:
\begin{itemize}
  \item \(\mathcal{G} \in \mathbb{R}^{R_1 \times R_2 \times R_3}\) is the \textbf{core tensor},
  \item \(U^{(1)} \in \mathbb{R}^{I \times R_1}\), \(U^{(2)} \in \mathbb{R}^{J \times R_2}\), and \(U^{(3)} \in \mathbb{R}^{K \times R_3}\) are the \textbf{factor matrices},
  \item \(\times_n\) denotes the mode-\(n\) tensor-matrix product.
\end{itemize}

In index notation, this becomes:

\[
\mathcal{\varphi}_{i,j,k} = \sum_{\alpha=1}^{R_1} \sum_{\beta=1}^{R_2} \sum_{\gamma=1}^{R_3}
\mathcal{G}_{\alpha,\beta,\gamma} \cdot U^{(1)}_{i,\alpha} \cdot U^{(2)}_{j,\beta} \cdot U^{(3)}_{k,\gamma}.
\]

With this formulation the parameters $R_1, R_2, R_3$ are set by the user and define the ``storage space'' in which the 
information required to represent $\varphi$ has to be condensed. It is proven that for $R_i = L,H,W$ respectively, the 
tensor $\varphi$ is exactly represented. However, being the goal a spare representation of the object's phase these numbers 
are chosen significantly smaller than any of the sizes of the array. The Tensorflow implementation of the Tucker decompostion 
is rather straightforward as the function \texttt{tf.einsum()} takes care of the tensor contraction.\\

A different approach that has been considered, leverages the TV regularization to push the algorithm towards a smooth object's 
phase. The full $L\times H\times W$ tensor is therefore optimized and a penalty on the sum of the absolute value of the 
gradients of the phase is added to the loss function. Precisely, the formula that has been used calculates the sum of the 
\textit{squared} gradients, since the square root operation, necessary to obtain the correct formula, creates 
problem around zero because of the infinite gradient. The final equation is therefore: 

\begin{equation}
\centering
   TV =  \alpha \sum_{i = 1}^{L}\sum_{j = 1}^{H}\sum_{k = 1}^{W} \mathcal S[(\varphi_{i} - \varphi_{i-1})^2 + \varphi_{j} - \varphi_{j-1})^2 +  \varphi_{k} - \varphi_{k-1})^2]
\end{equation}

where $\alpha $ is a hyperparameter that acts as a scaling factor, $(i,j,k)$ are the indices running over the coordinates of
the $L\times H\times W$ grid and $\mathcal S$ is the object support. The parameter $\alpha $ in this case was chosen to be assigned as a fraction, imposed by the user, 
of the value of the data fidelity loss. Further developments could aim at finding adaptive formulations for the magnitude 
of $\alpha $. 

\subsection{Loss function}
Another important aspect of the model is the loss function. Typically, for inverse problems there is \textit{data fidelity}
term that in this case measures the distance between $I_{obs}$ and $ I_{calc}$ according to some metric, and other additional 
\textit{regularization} terms that guide the optimization process with physical constraints. 

\textbf{Data fidelity}: 
The most common and intuitive metrics are the Mean Squared Error (MSE) and the Mean Absolute Error (MAE) that evaluate 
the Euclidean distance between the observed and calculated intensities. Practically, because of the large dynamic range 
of typical BCDI data, the MAE performs better as it doesn't focus on bright pixels only, but manages to correct for lower 
intensity tails as well. 
A more faithful metric for BCDI experimental data is the Poisson Negative Log-Likelihood (P-NLLK). This metric assumes 
indeed the handling of count data, like the type obtained by photon counting detectors, and that the stochasticity of physical 
process is Poisson distributed. When summed over the full dataset, the discrepancies between calculated and observed 
intensities are not intended as Euclidean distances but like divergences between two probability distributions. In other 
words, the P-NLLK estimates the likelihood that $I_{calc}$ belongs to the same Poisson distribution of $I_{obs}$ \cite{Thibault_2012}. 
Derived from the equation for the probability for Poissonian events, the formula of the averaged P-NLLK, in the form of 
a Kullback-Liebler divergence is: 

\begin{equation}
    \bigl\langle \mathrm{P-LLK}\bigr\rangle
    = \frac{2}{N_{\mathrm{obs}}}
    \left[
      \sum_{I_{\rm obs}>0}
        \bigl(
          I_{\rm calc} - I_{\rm obs}
          + I_{\rm obs}\,\ln\!\frac{I_{\rm obs}}{I_{\rm calc}}
        \bigr)
      \;+\;
      \sum_{I_{\rm obs}=0} I_{\rm calc}
    \right].
\end{equation}
    

Both the MAE and the P-NLLK have been tested on several simulated and experimental datasets and the MAE has always shown 
better convergence. An explanation for this unexpected result is yet to be found, but the main suspect is that the gradients 
calculated during the backpropagation can have instabilities because of the logarithm. 

\textbf{Regularizations}:
Beside the TV on the object's phase to ensure smoothness, another term that was considered concerns the size of the support. 
For a given data fidelity value, it is known that the object with the smallest support represents the optimal solution 
\cite{favre-nicolin_free_2020}. Intuitively this could be explained with the fact that there are many more object phase configuration 
that would combine constructive and destructive interferences to match the observed intensity in reciprocal space. 
The analogous measure is the shrinkwrap algorithm \cite{Marchesini_shrinkwrap} utilized in alternating projections algorithms.  
For this reason a penalty $P$ on the size of the support can be added to the loss function with the formula: 

\begin{equation}
    \centering
    P = \beta  \sum_{i,j,k} \mathcal S
\end{equation}

where $\beta$ is a hyperparameter that similarly to $\alpha$ is chosen by the user with respect to the data fidelity loss. 
Both the hyperparameters can be tuned manually during the optimization, to adjust in case of need, the strength of 
the regularization terms. 
The final formula for the loss function can be ultimately expressed as: 

\begin{equation}
       L = \frac{1}{N_{\mathrm{obs}}}
       \sum_{i=1}^{N_{\mathrm{obs}}}
       \bigl\lvert I_{\mathrm{calc},i} \;-\; I_{\mathrm{obs},i}\bigr\rvert  
        + \alpha TV(\varphi)
        + \beta  P(\mathcal S)
\end{equation}

For the optimization, a tolerance on the MAE value or a fixed number of steps can be set to stop the algorithm. Empirical 
observations have shown that a MAE value around 0.2 is sufficiently low for the result to be considered good. However, 
an additional refinement with a few iterations (~ 300) is recommended for cross-validation.  \\

Before concluding the paragraph, it is worth highlighting that this AD implementation offers the possibility to simultaneously 
run multiple reconstructions in parallel, efficiently on the GPU. One can create a 4D tensor by stacking several 3D 
intensity data, creating therefore a batch. For each element in the batch a different initial support and phase configuration 
can be chosen, hence increasing the likelihood to converge to the solution. 

\section{Results}
In this section some relevant results will be presented. 

%  find a dataset: --> 

% show pynx 
% show DL + PyNX
% show AD 

%  find a dataset with dislocations: mouad266

% show pynx
% show DL + PyNX
% show AD 

\subsection{Low-strain case}
\subsection{High-strain case}

\section{Conclusions}


